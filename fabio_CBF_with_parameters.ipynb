{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import *\n",
    "from scipy.sparse.linalg import svds\n",
    "import math\n",
    "\n",
    "from recsys.preprocess import *\n",
    "\n",
    "import functools\n",
    "\n",
    "from recsys.utility import *\n",
    "\n",
    "#RANDOM_STATE = 666\n",
    "\n",
    "#np.random.seed(RANDOM_STATE)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(train, test_size=0.3, min_playlist_tracks=7):\n",
    "    \"\"\"\n",
    "        Standard train_test_split, no modifications.\n",
    "    \"\"\"\n",
    "    playlists = train.groupby('playlist_id').count()\n",
    "\n",
    "    # Only playlists with at least \"min_playlist_tracks\" tracks are considered.\n",
    "    # If \"min_playlists_tracks\" = 7, then 28311 out of 45649 playlists in \"train\" are considered.\n",
    "    to_choose_playlists = playlists[playlists['track_id'] >= min_playlist_tracks].index.values\n",
    "\n",
    "\n",
    "    # Among these playlists, \"test_size * len(to_choose_playlists)\" distinct playlists are chosen for testing.\n",
    "    # If \"test_size\" = 0.3, then 8493 playlists are chosen for testing.\n",
    "    # It's a numpy array that contains playlis_ids.\n",
    "    target_playlists = np.random.choice(to_choose_playlists, replace=False, size=int(test_size * len(to_choose_playlists)))\n",
    "\n",
    "    target_tracks = np.array([])\n",
    "    indexes = np.array([])\n",
    "    for p in target_playlists:\n",
    "        # Choose 5 random tracks of such playlist: since we selected playlists with at least \"min_playlist_tracks\"\n",
    "        # tracks, if \"min_playlist_tracks\" is at least 5, we are sure to find them.\n",
    "        selected_df = train[train['playlist_id'] == p].sample(5)\n",
    "\n",
    "        selected_tracks = selected_df['track_id'].values\n",
    "        target_tracks = np.union1d(target_tracks, selected_tracks)\n",
    "        indexes = np.union1d(indexes, selected_df.index.values)\n",
    "\n",
    "    test = train.loc[indexes].copy()\n",
    "    train = train.drop(indexes)\n",
    "\n",
    "    return train, test, pd.DataFrame(target_playlists, columns=['playlist_id']), pd.DataFrame(target_tracks, columns=['track_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import *\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def dot_with_top(m1, m2, def_rows_g, top=-1, row_group=1, similarity=\"dot\", shrinkage=0):\n",
    "    \"\"\"\n",
    "        Produces the product between matrices m1 and m2.\n",
    "        Possible similarities: \"dot\", \"cosine\". By default it goes on \"dot\".\n",
    "        NB: Shrinkage is not implemented...\n",
    "        Code taken from\n",
    "            https://stackoverflow.com/questions/29647326/sparse-matrix-dot-product-keeping-only-n-max-values-per-result-row\n",
    "            and optimized for smart dot products.\n",
    "    \"\"\"\n",
    "    m2_transposed = m2.transpose()\n",
    "    \n",
    "    if top > 0:\n",
    "        final_rows = []\n",
    "        row_id = 0\n",
    "        while row_id < m1.shape[0]:\n",
    "            last_row = row_id + row_group if row_id + row_group <= m1.shape[0] else m1.shape[0]\n",
    "            rows = m1[row_id:last_row]\n",
    "            if rows.count_nonzero() > 0:\n",
    "                if similarity == \"cosine\":\n",
    "                    res_rows = cosine_similarity(rows, m2_transposed, dense_output=False)\n",
    "                else:\n",
    "                    res_rows = rows.dot(m2)\n",
    "                if shrinkage > 0:\n",
    "                    res_rows = apply_shrinkage(rows, res_rows, shrinkage)\n",
    "                if res_rows.count_nonzero() > 0:\n",
    "                    for res_row in res_rows:\n",
    "                        if res_row.nnz > top:\n",
    "                            args_ids = np.argsort(res_row.data)[-top:]\n",
    "                            data = res_row.data[args_ids]\n",
    "                            cols = res_row.indices[args_ids]\n",
    "                            final_rows.append(csr_matrix((data, (np.zeros(top), cols)), shape=res_row.shape))\n",
    "                        else:\n",
    "                            final_rows.append(def_rows_g[0])\n",
    "                else:\n",
    "                    for res_row in res_rows:\n",
    "                        final_rows.append(def_rows_g[0])\n",
    "            else:\n",
    "                final_rows.append(def_rows_g)\n",
    "            row_id += row_group\n",
    "            if row_id % row_group == 0:\n",
    "                print(row_id)\n",
    "        return scipy.sparse.vstack(final_rows, 'csr')\n",
    "    return m1.dot(m2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions(test=None, target_playlists=None, compute_MAP=False, row_group=100):\n",
    "    \"\"\"\n",
    "        Produces a prediction dataframe for \"test\", where each row corresponds to a playlist in \"target_playlists\".\n",
    "        If compute_MAP is true, then it print the MAP every \"row_group\" playlists.\n",
    "        It's optimized for doing dot products for different playlist at once.\n",
    "            \"row_group\" is the number of playlists in each of these optimized dot products.\n",
    "            The higher is row_group, the faster are the predictions but more memory is used.\n",
    "    \"\"\"\n",
    "    # Create predictions dataframe\n",
    "    predictions = pd.DataFrame(target_playlists)\n",
    "    predictions.index = target_playlists['playlist_id']\n",
    "    predictions['track_ids'] = [np.array([]) for i in range(len(predictions))]\n",
    "    ttracks = set(target_tracks['track_id'].values)\n",
    "    if compute_MAP:\n",
    "        test_good = get_playlist_track_list2(test)\n",
    "        test_good.index = test_good.playlist_id.apply(lambda pl_id: playlist_to_num[pl_id])\n",
    "        print(len(test_good))\n",
    "    \n",
    "    # This is the sum of all the AP of the playlists.\n",
    "    # When we print the MAP, we divide \"sum_ap\" by the number of considered playlists.\n",
    "    sum_ap = 0\n",
    "    \n",
    "    # Let's start the predictions!\n",
    "    row_start = 0\n",
    "    while row_start < len(target_playlists):\n",
    "        # We'll do dot products for all playlists in \"target_playlists\" from \"row_start\" to \"row_end\"\n",
    "        row_end = row_start + row_group if row_start + row_group <= len(target_playlists) else len(target_playlists)\n",
    "        \n",
    "        # \"pl_group\" is the set of the playlists that we want to make prediction for\n",
    "        pl_group = target_playlists[row_start:row_end]\n",
    "        \n",
    "        # Now we need to build a matrix where, for each playlist in \"pl_group\", we take the correspondent URM row slice\n",
    "        rows_URM_sqrt = []\n",
    "        for pl_id in pl_group.playlist_id:\n",
    "            rows_URM_sqrt += [URM_sqrt[pl_id,:]]\n",
    "        composed_URM_sqrt = scipy.sparse.vstack(rows_URM_sqrt, 'csr')\n",
    "        \n",
    "        # Compute predictions for current playlist group: here we do all the smart dot products...\n",
    "        # \"simil_ii\" are the scores for playlists in common\n",
    "        # \"simil_album\" and \"simil_artist\" are scores for albums and artists (captain obvious)\n",
    "        #simil_ii = np.array(np.divide(TTM.dot(composed_URM_sqrt.transpose()).transpose().todense(), TTM.sum(axis=1).transpose()))\n",
    "        simil_album = np.array(np.divide(SYM_ALBUM.dot(composed_URM_sqrt.transpose()).transpose().todense(), SYM_ALBUM.sum(axis=1).transpose()))\n",
    "        simil_artist = np.array(np.divide(SYM_ARTIST.dot(composed_URM_sqrt.transpose()).transpose().todense(), SYM_ARTIST.sum(axis=1).transpose()))\n",
    "\n",
    "        # Now we should consider one playlist at a time, take its own personalized parameters and make the prediction\n",
    "        for i,pl_id in enumerate(pl_group.playlist_id):\n",
    "            # Retrieve parameters\n",
    "            ii_param = playlist_params.loc[pl_id].ii_param_norm\n",
    "            album_param = playlist_params.loc[pl_id].album_param_norm\n",
    "            artist_param = playlist_params.loc[pl_id].artist_param_norm\n",
    "            \n",
    "            # Tracks that we know are in the playlist (so we shouldn't recommend them)\n",
    "            pl_tracks = set(playlist_tracks.loc[pl_id]['track_ids'])\n",
    "\n",
    "            pred = []\n",
    "            \n",
    "            # If you want to do some testing only on specific features, put the weights of the other features to zero.\n",
    "            #ii_param = 0\n",
    "            #album_param = 1\n",
    "            #artist_param = 0\n",
    "            \n",
    "            # Combine all the predictions and sort them from best to worst\n",
    "            simil = ii_param * simil_ii[i] + album_param * simil_album[i] + artist_param * simil_artist[i]\n",
    "            sorted_ind = simil.argsort()[::-1]\n",
    "\n",
    "            # Predict...\n",
    "            i = 0\n",
    "            c = 0\n",
    "            while i < len(sorted_ind) and c < 5:\n",
    "                tr = sorted_ind[i]\n",
    "                if (tr in ttracks) and (tr not in pl_tracks):\n",
    "                    pred.append(num_to_tracks[tr])\n",
    "                    c+=1\n",
    "                i+=1\n",
    "            predictions.loc[pl_id] = predictions.loc[pl_id].set_value('track_ids', np.array(pred))\n",
    "            \n",
    "            # Update MAP\n",
    "            if compute_MAP:\n",
    "                correct = 0\n",
    "                ap = 0\n",
    "                for it, t in enumerate(pred):\n",
    "                    tr_ids = test_good.loc[pl_id]['track_ids']\n",
    "                    if t in tr_ids:\n",
    "                        correct += 1\n",
    "                        ap += correct / (it+1)\n",
    "                ap /= len(pred)\n",
    "                sum_ap += ap\n",
    "        \n",
    "        # Update \"row_start\" to \"row_end\" and proceed to next pl_group\n",
    "        row_start = row_end\n",
    "        \n",
    "        print(row_start)\n",
    "        if compute_MAP:\n",
    "            print(sum_ap / row_start)\n",
    "            \n",
    "    #predictions['playlist_id'] = predictions['playlist_id_tmp']\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_num_to_id(df, row_num, column = 'track_id'):\n",
    "    \"\"\" df must have a 'track_id' column \"\"\"\n",
    "    return df.iloc[row_num][column]\n",
    "\n",
    "def from_id_to_num(df, tr_id, column='track_id'):\n",
    "    \"\"\" df must have a 'track_id' column \"\"\"\n",
    "    return np.where(df[column].values == tr_id)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_final.csv', delimiter='\\t')\n",
    "playlists = pd.read_csv('data/playlists_final.csv', delimiter='\\t')\n",
    "target_playlists = pd.read_csv('data/target_playlists.csv', delimiter='\\t')\n",
    "target_tracks = pd.read_csv('data/target_tracks.csv', delimiter = '\\t')\n",
    "tracks = pd.read_csv('data/tracks_final.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We load them just to compare the ones for testing with the original ones.\n",
    "# NB: we shouldn't use them in training!\n",
    "train_original = pd.read_csv('data/train_final.csv', delimiter='\\t')\n",
    "target_playlists_original = pd.read_csv('data/target_playlists.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train), len(target_playlists), len(target_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, target_playlists, target_tracks = train_test_split(train, test_size=0.4, min_playlist_tracks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train), len(test), len(target_playlists), len(target_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_target_playlists = pd.read_csv('data/target_playlists.csv', delimiter='\\t')\n",
    "print(\"Number of playlists in the new target_playlists that are also in the original target_playlists\")\n",
    "print(len(target_playlists[target_playlists.playlist_id.isin(full_target_playlists.playlist_id)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Almost all of these were taken from one of your notebook, so you probably understand them\n",
    "tracks['track_id_tmp'] = tracks['track_id']\n",
    "\n",
    "tracks['track_id'] = tracks.index\n",
    "\n",
    "playlists['playlist_id_tmp'] = playlists['playlist_id']\n",
    "playlists['playlist_id'] = playlists.index\n",
    "\n",
    "train['playlist_id_tmp'] = train['playlist_id']\n",
    "train['track_id_tmp'] = train['track_id']\n",
    "\n",
    "track_to_num = pd.Series(tracks.index)\n",
    "track_to_num.index = tracks['track_id_tmp']\n",
    "\n",
    "playlist_to_num = pd.Series(playlists.index)\n",
    "playlist_to_num.index = playlists['playlist_id_tmp']\n",
    "\n",
    "num_to_tracks = pd.Series(tracks['track_id_tmp'])\n",
    "\n",
    "\n",
    "train['track_id'] = train['track_id'].apply(lambda x : track_to_num[x])\n",
    "train['playlist_id'] = train['playlist_id'].apply(lambda x : playlist_to_num[x])\n",
    "\n",
    "tracks.tags = tracks.tags.apply(lambda s: np.array(eval(s), dtype=int))\n",
    "\n",
    "playlists.title = playlists.title.apply(lambda s: np.array(eval(s), dtype=int))\n",
    "\n",
    "target_playlists['playlist_id_tmp'] = target_playlists['playlist_id']\n",
    "target_playlists['playlist_id'] = target_playlists['playlist_id'].apply(lambda x : playlist_to_num[x])\n",
    "\n",
    "target_tracks['track_id_tmp'] = target_tracks['track_id']\n",
    "target_tracks['track_id'] = target_tracks['track_id'].apply(lambda x : track_to_num[x])\n",
    "\n",
    "# Create a dataframe that maps a playlist to the set of its tracks\n",
    "playlist_tracks = pd.DataFrame(train['playlist_id'].drop_duplicates())\n",
    "playlist_tracks.index = train['playlist_id'].unique()\n",
    "playlist_tracks['track_ids'] = train.groupby('playlist_id').apply(lambda x : x['track_id'].values)\n",
    "playlist_tracks = playlist_tracks.sort_values('playlist_id')\n",
    "\n",
    "# Create a dataframe that maps a track to the set of the playlists it appears into\n",
    "track_playlists = pd.DataFrame(train['track_id'].drop_duplicates())\n",
    "track_playlists.index = train['track_id'].unique()\n",
    "track_playlists['playlist_ids'] = train.groupby('track_id').apply(lambda x : x['playlist_id'].values)\n",
    "track_playlists = track_playlists.sort_values('track_id')\n",
    "\n",
    "# Substitute each bad album (i.e. an illformed album such as -1, None, etc) to a new album\n",
    "bad_albums = 0\n",
    "def transform_album_1(alb):\n",
    "    global bad_albums\n",
    "    ar = eval(alb)\n",
    "    if len(ar) == 0 or (len(ar) > 0 and (ar[0] == None or ar[0] == -1)):\n",
    "        ar = [-1]\n",
    "        bad_albums += 1\n",
    "    return ar[0]\n",
    "def transform_album_2(alb):\n",
    "    global next_album_id\n",
    "    if alb == -1:\n",
    "        alb = next_album_id\n",
    "        next_album_id += 1\n",
    "    return alb\n",
    "tracks.album = tracks.album.apply(lambda alb: transform_album_1(alb))\n",
    "last_album = tracks.album.max()\n",
    "next_album_id = last_album + 1\n",
    "tracks.album = tracks.album.apply(lambda alb: transform_album_2(alb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target playlists analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II\n",
    "\"II\" means Item-Item collaborative filtering, i.e. playlists in common...\n",
    "\n",
    "Steps:\n",
    "1 - Create a URM (URM_sqrt) normalized with a modified IDF which has a sqrt.\n",
    "2 - Compute TTM as URM_sqrt.dot(URM_sqrt.transpose()). Keep the K best for each row.\n",
    "3 - Compute personalized parameters for each playlist. Here we compute the ii_parameter, which indicates how much a playlist is affine to be predicted using the TTM. This is done by doing the following things for each row:\n",
    "    - compute a np.array by doing the sum of all the rows in the TTM that corresponds to a track in the considered playlist\n",
    "    - compute the ii_parameter of the playlist by doing 1/(entropy_of_the_computed_array + 0.05). \"0.05\" is needed since it may happens that the entropy is zero and so the ratio goes to infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# User Rating Matrix URM\n",
    "def get_URM(tracks, playlists, playlist_tracks, track_playlists, norm=\"no\"):\n",
    "    \"\"\"\n",
    "        possible normalizations: \"no\", \"magnitude\", \"idf\", \"sqrt\". Default \"no\".\n",
    "    \"\"\"\n",
    "    URM = lil_matrix((len(playlists), len(tracks)))\n",
    "    num_playlists = len(playlist_tracks)\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for row in track_playlists.itertuples():\n",
    "        track_id = row.track_id\n",
    "        nq = len(row.playlist_ids)\n",
    "        for pl_id in row.playlist_ids:\n",
    "            if norm == \"idf\":\n",
    "                URM[pl_id,track_id] = math.log((num_playlists - nq + 0.5)/(nq + 0.5))\n",
    "            elif norm == \"sqrt\":\n",
    "                URM[pl_id,track_id] = math.sqrt((num_playlists - nq + 0.5)/(nq + 0.5))\n",
    "            else:\n",
    "                URM[pl_id,track_id] = 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "        \n",
    "    if norm == \"magnitude\":\n",
    "        for pl_id in playlists.playlist_id:\n",
    "            magnitude = math.sqrt(len(URM.data[pl_id]))\n",
    "            for col in URM.rows[pl_id]:\n",
    "                URM[pl_id,col] /= magnitude\n",
    "    \n",
    "    return URM\n",
    "\n",
    "#\n",
    "# URM:\n",
    "# \n",
    "#              tracks\n",
    "#            _________\n",
    "#           \\         \\\n",
    "# playlists \\         \\\n",
    "#           \\_________\\\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#URM_no_norm = get_URM(tracks, playlists, playlist_tracks, track_playlists, norm=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URM_sqrt = get_URM(tracks, playlists, playlist_tracks, track_playlists, norm=\"sqrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: produce item-item matrix with cosine similarity\n",
    "row_group = 10000\n",
    "def_rows_i = URM_sqrt.transpose()[0:row_group].dot(URM_sqrt) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "TTM = dot_with_top(URM_sqrt.transpose(), URM_sqrt, def_rows_i, top=100, row_group=row_group, similarity=\"cosine\", shrinkage=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "playlist_params = pd.DataFrame(playlist_tracks.playlist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: compute how much each playlist is affine to be classified with such similarity concept\n",
    "playlist_params['ii_param'] = 0.0\n",
    "\n",
    "counter = 0\n",
    "for pl_id in playlist_tracks[playlist_tracks.playlist_id.isin(target_playlists.playlist_id)].playlist_id:\n",
    "    trks = playlist_tracks.loc[pl_id].track_ids\n",
    "    tot = np.zeros((1,TTM.shape[0]))[0]\n",
    "    for tr_id in trks:\n",
    "        tot += TTM[tr_id].toarray()[0]\n",
    "    v = 1 / (scipy.stats.entropy(tot + 0.05))\n",
    "    playlist_params.set_value(pl_id, \"ii_param\", v)\n",
    "    counter += 1\n",
    "    if counter % 500 == 0:\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "playlist_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Album\n",
    "\n",
    "<div style=\"white-space: pre-wrap;\">\n",
    "Steps:\n",
    "1 - Compute the playlists_x_albums (i.e. the UAM_album matrix, where U stands for User) sparse matrix. I do this before computing the tracks_x_albums (i.e. the IAM_album matrix, where I stands for Item) sparse matrix because here I compute also the \"album_to_val\" dictionary, which contains the IDF value of each album obtained considering the playlists as document (and not the tracks). However at the moment I don't use this because I compute the IAM_album matrix without any normalization, so you may skip it...\n",
    "2 - Compute the tracks_x_albums IAM_album sparse matrix.\n",
    "3 - Compute the SYM_ALBUM tracks_x_tracks matrix by doing IAM_album.dot(IAM_album.transpose()). It's not big, so I don't need to keep the K best values...\n",
    "4 - Compute the album_parameter, which means \"how much each playlist is affine to album similarity\". I do this by computing the entropy of the numpy array containing the occurrences of the albums in the playlist, and then doing 1/(entropy_of_array + 0.05).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_UAM_album(tracks, playlist_tracks, target_playlists, norm=\"no\", OKAPI_K=1.7, OKAPI_B=0.75):\n",
    "    \"\"\"\n",
    "        Possible norms are \"no\", \"idf\", okapi\". Default to \"no\".\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_albums = tracks.album.unique()\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    UAM_album = lil_matrix((max(playlists.playlist_id)+1, max(unique_albums)+1))\n",
    "    UAM_album_no_norm = lil_matrix((max(playlists.playlist_id)+1, max(unique_albums)+1))\n",
    "    album_to_playlists = {}\n",
    "    \n",
    "    for row in playlist_tracks.itertuples():\n",
    "        pl_id = row.playlist_id\n",
    "        for tr_id in row.track_ids:\n",
    "            alb = tracks.loc[tr_id].album\n",
    "            UAM_album[pl_id,alb] += 1\n",
    "            UAM_album_no_norm[pl_id,alb] += 1\n",
    "            if alb not in album_to_playlists:\n",
    "                album_to_playlists[alb] = [pl_id]\n",
    "            else:\n",
    "                album_to_playlists[alb].append(pl_id)\n",
    "                \n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    album_to_val = {}\n",
    "    if norm == \"okapi\" or norm == \"idf\":\n",
    "        avg_document_length = functools.reduce(lambda acc,tr_ids: acc + len(tr_ids), playlist_tracks.track_ids, 0) / len(playlist_tracks)\n",
    "        N = len(playlist_tracks)\n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        for row in playlist_tracks.itertuples():\n",
    "            pl_id = row.playlist_id\n",
    "            albums = UAM_album.rows[pl_id]\n",
    "            data = UAM_album.data[pl_id]\n",
    "            for album in albums:\n",
    "                fq = UAM_album[pl_id,album]\n",
    "                nq = len(album_to_playlists[album])\n",
    "                idf = math.log((N - nq + 0.5)/(nq + 0.5))\n",
    "                \n",
    "                if album not in album_to_val:\n",
    "                    album_to_val[album] = idf\n",
    "                    \n",
    "                if norm == \"idf\":\n",
    "                    UAM_album[pl_id,album] = idf\n",
    "                elif norm == \"okapi\":\n",
    "                    UAM_album[pl_id,album] = idf*(fq*(OKAPI_K+1))/(fq + OKAPI_K*(1 - OKAPI_B + OKAPI_B * sum(data) / avg_document_length))\n",
    "            i += 1\n",
    "            if i % 1000 == 0:\n",
    "                print(i)\n",
    "    \n",
    "    return UAM_album, UAM_album_no_norm, album_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "UAM_album, UAM_album_no_norm, album_to_val = get_UAM_album(tracks, playlist_tracks, target_playlists, norm=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_albums = tracks.album.unique()\n",
    "unique_albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_IAM_album(tracks, target_tracks, norm=\"no\"):\n",
    "    \"\"\"\n",
    "        Possible norms are \"no\", \"idf\". Default \"no\".\n",
    "    \"\"\"\n",
    "    unique_albums = tracks.album.unique()\n",
    "    IAM_album = lil_matrix((len(tracks), max(unique_albums)+1))\n",
    "    \n",
    "    num_tracks = len(tracks)\n",
    "    i = 0\n",
    "    \n",
    "    for row in tracks.itertuples():\n",
    "        nq = 1\n",
    "        if norm == \"idf\":\n",
    "            if row.album in album_to_val:\n",
    "                IAM_album[row.track_id,row.album] = album_to_val[row.album]\n",
    "            else:\n",
    "                IAM_album[row.track_id,row.album] = 0 # Give zero if the album is not in any playlist!\n",
    "        else:\n",
    "            IAM_album[row.track_id,row.album] = 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "    \n",
    "    return IAM_album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "IAM_album = get_IAM_album(tracks, target_tracks, norm=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3\n",
    "SYM_ALBUM = IAM_album.dot(IAM_album.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 4: compute how much each user is affine to album similarity\n",
    "playlist_params['album_param'] = 0.0\n",
    "\n",
    "UAM_album_no_norm_csc = UAM_album_no_norm.tocsr()\n",
    "counter = 0\n",
    "for pl_id in playlist_tracks[playlist_tracks.playlist_id.isin(target_playlists.playlist_id)].playlist_id:\n",
    "    v = 1 / (scipy.stats.entropy(UAM_album_no_norm_csc.getrow(pl_id).data) + 0.05)\n",
    "    playlist_params.set_value(pl_id, \"album_param\", v)\n",
    "    counter += 1\n",
    "    if counter % 5000 == 0:\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "playlist_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artist\n",
    "Same steps as for Album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# User Artist Matrix UAM\n",
    "def get_UAM(tracks, playlist_tracks, target_playlists, norm=\"no\", OKAPI_K=1.7, OKAPI_B=0.75):\n",
    "    \"\"\"\n",
    "        Possible norms are \"no\", \"idf\", okapi\". Default to \"no\".\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_artists = tracks.artist_id.unique()\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    UAM = lil_matrix((max(playlists.playlist_id)+1, max(unique_artists)+1))\n",
    "    UAM_no_norm = lil_matrix((max(playlists.playlist_id)+1, max(unique_artists)+1))\n",
    "    artist_to_playlists = {}\n",
    "    \n",
    "    for row in playlist_tracks.itertuples():\n",
    "        pl_id = row.playlist_id\n",
    "        for tr_id in row.track_ids:\n",
    "            art = tracks.loc[tr_id].artist_id\n",
    "            UAM[pl_id,art] += 1\n",
    "            UAM_no_norm[pl_id,art] += 1\n",
    "            if art not in artist_to_playlists:\n",
    "                artist_to_playlists[art] = [pl_id]\n",
    "            else:\n",
    "                artist_to_playlists[art].append(pl_id)\n",
    "                \n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    artist_to_val = {}\n",
    "    if norm == \"okapi\" or norm == \"idf\":\n",
    "        avg_document_length = functools.reduce(lambda acc,tr_ids: acc + len(tr_ids), playlist_tracks.track_ids, 0) / len(playlist_tracks)\n",
    "        N = len(playlist_tracks)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for row in playlist_tracks.itertuples():\n",
    "            pl_id = row.playlist_id\n",
    "            artists = UAM.rows[pl_id]\n",
    "            data = UAM.data[pl_id]\n",
    "            for artist in artists:\n",
    "                fq = UAM[pl_id,artist]\n",
    "                nq = len(artist_to_playlists[artist])\n",
    "                idf = math.log((N - nq + 0.5)/(nq + 0.5))\n",
    "                \n",
    "                if artist not in artist_to_val:\n",
    "                    artist_to_val[artist] = idf\n",
    "                \n",
    "                if norm == \"idf\":\n",
    "                    UAM[pl_id,artist] = idf\n",
    "                else:\n",
    "                    UAM[pl_id,artist] = idf*(fq*(OKAPI_K+1))/(fq + OKAPI_K*(1 - OKAPI_B + OKAPI_B * sum(data) / avg_document_length))\n",
    "            i += 1\n",
    "            if i % 1000 == 0:\n",
    "                print(i)\n",
    "    \n",
    "    return UAM, UAM_no_norm, artist_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "UAM, UAM_no_norm, artist_to_val = get_UAM(tracks, playlist_tracks, target_playlists, norm=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_artists = tracks.artist_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Item Artist Matrix\n",
    "def get_IAM(tracks, target_tracks, norm=\"no\"):\n",
    "    \"\"\"\n",
    "        Possible norms are \"no\", \"idf\". Default to \"no\".\n",
    "    \"\"\"\n",
    "    unique_artists = tracks.artist_id.unique()\n",
    "    IAM = lil_matrix((len(tracks), max(unique_artists)+1))\n",
    "    \n",
    "    num_tracks = len(tracks)\n",
    "    i = 0\n",
    "    \n",
    "    for row in tracks.itertuples():\n",
    "        if norm == \"idf\":\n",
    "            if row.artist_id in artist_to_val:\n",
    "                IAM[row.track_id,row.artist_id] = artist_to_val[row.artist_id]\n",
    "            else:\n",
    "                IAM[row.track_id,row.artist_id] = 0 # Give zero if the album is not in any playlist!\n",
    "        else:\n",
    "            IAM[row.track_id,row.artist_id] = 1\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "    \n",
    "    return IAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "IAM = get_IAM(tracks, target_tracks, norm=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3\n",
    "SYM_ARTIST = IAM.dot(IAM.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UAM_csc = UAM.tocsc()\n",
    "UAM_no_norm_csc = UAM_no_norm.tocsc()\n",
    "IAM_csr_transpose = IAM.tocsr().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 4: compute how much each user is affine to artist similarity\n",
    "playlist_params['artist_param'] = 0.0\n",
    "\n",
    "counter = 0\n",
    "for pl_id in playlist_tracks[playlist_tracks.playlist_id.isin(target_playlists.playlist_id)].playlist_id:\n",
    "    v = 1 / (scipy.stats.entropy(UAM_no_norm_csc.getrow(pl_id).data) + 0.05)\n",
    "    playlist_params.set_value(pl_id, \"artist_param\", v)\n",
    "    counter += 1\n",
    "    if counter % 5000 == 0:\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "playlist_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust params\n",
    "\n",
    "Here I just want to adjust the parameters so that they can be compaired... I do zeta scoring to them and put the means to 1. We should do some more experiments here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Execute only once\n",
    "playlist_params_copy = playlist_params.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Restore playlist_params\n",
    "playlist_params = playlist_params_copy.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "playlist_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "playlist_params[\"ii_param_norm\"] = playlist_params.ii_param.clip(0)\n",
    "playlist_params[\"album_param_norm\"] = playlist_params.album_param.clip(0)\n",
    "playlist_params[\"artist_param_norm\"] = playlist_params.artist_param.clip(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#playlist_params[\"ii_param_norm\"] = np.sqrt(playlist_params.ii_param_norm)\n",
    "#playlist_params[\"album_param_norm\"] = np.sqrt(playlist_params.album_param_norm)\n",
    "#playlist_params[\"artist_param_norm\"] = np.sqrt(playlist_params.artist_param_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "playlist_params.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "playlist_params[\"ii_param_norm\"] = ((playlist_params.ii_param_norm - playlist_params.ii_param_norm.mean()) / playlist_params.ii_param_norm.std()) + 1\n",
    "playlist_params[\"album_param_norm\"] = ((playlist_params.album_param_norm - playlist_params.album_param_norm.mean()) / playlist_params.album_param_norm.std()) + 1\n",
    "playlist_params[\"artist_param_norm\"] = ((playlist_params.artist_param_norm - playlist_params.artist_param_norm.mean()) / playlist_params.artist_param_norm.std()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#playlist_params.album_param_norm -= 0.2\n",
    "#playlist_params.artist_param_norm -= 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "playlist_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ii_param wins against other parameters\n",
    "len(playlist_params[(playlist_params.ii_param_norm >= playlist_params.album_param_norm) & (playlist_params.ii_param_norm >= playlist_params.artist_param_norm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# album_param wins\n",
    "len(playlist_params[(playlist_params.album_param_norm >= playlist_params.ii_param_norm) & (playlist_params.album_param_norm >= playlist_params.artist_param_norm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# artist_param wins\n",
    "len(playlist_params[(playlist_params.artist_param_norm >= playlist_params.album_param_norm) & (playlist_params.artist_param_norm >= playlist_params.ii_param_norm)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(test[test.playlist_id.isin(target_playlists_original.playlist_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predictions for all the playlists in test\n",
    "make_predictions(test=test, target_playlists=target_playlists, compute_MAP=True, row_group=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predictions for playlists in test that were also in the original target_playlists\n",
    "predictions = make_predictions(test=test[test.playlist_id.isin(target_playlists_original.playlist_id)],\n",
    "                               target_playlists=target_playlists[target_playlists.playlist_id_tmp.isin(target_playlists_original.playlist_id)],\n",
    "                               compute_MAP=True, row_group=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predictions for playlists in test not in the original target_playlists\n",
    "predictions = make_predictions(test=test[~test.playlist_id.isin(target_playlists_original.playlist_id)],\n",
    "                               target_playlists=target_playlists[~target_playlists.playlist_id_tmp.isin(target_playlists_original.playlist_id)],\n",
    "                               compute_MAP=True, row_group=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions['playlist_id'] = predictions['playlist_id_tmp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate(test[~test.playlist_id.isin(target_playlists_original.playlist_id)], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = predictions.drop(\"playlist_id_tmp\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the dataframe friendly for output -> convert np.array in string\n",
    "predictions['track_ids'] = predictions['track_ids'].apply(lambda x : ' '.join(map(str, x)))\n",
    "predictions.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
