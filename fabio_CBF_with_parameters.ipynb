{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import *\n",
    "from scipy.sparse.linalg import svds\n",
    "import math\n",
    "\n",
    "from recsys.preprocess import *\n",
    "\n",
    "import functools\n",
    "\n",
    "#from recsys.utility import *\n",
    "\n",
    "#RANDOM_STATE = 666\n",
    "\n",
    "#np.random.seed(RANDOM_STATE)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_test_split(train, test_size=0.3, min_playlist_tracks=7):\n",
    "    \"\"\"\n",
    "        Standard train_test_split, no modifications.\n",
    "    \"\"\"\n",
    "    playlists = train[train.playlist_id.isin(target_playlists_original.playlist_id)].groupby('playlist_id').count()\n",
    "\n",
    "    # Only playlists with at least \"min_playlist_tracks\" tracks are considered.\n",
    "    # If \"min_playlists_tracks\" = 7, then 28311 out of 45649 playlists in \"train\" are considered.\n",
    "    to_choose_playlists = playlists[playlists['track_id'] >= min_playlist_tracks].index.values\n",
    "\n",
    "\n",
    "    # Among these playlists, \"test_size * len(to_choose_playlists)\" distinct playlists are chosen for testing.\n",
    "    # If \"test_size\" = 0.3, then 8493 playlists are chosen for testing.\n",
    "    # It's a numpy array that contains playlis_ids.\n",
    "    target_playlists = np.random.choice(to_choose_playlists, replace=False, size=int(test_size * len(to_choose_playlists)))\n",
    "\n",
    "    target_tracks = np.array([])\n",
    "    indexes = np.array([])\n",
    "    for p in target_playlists:\n",
    "        # Choose 5 random tracks of such playlist: since we selected playlists with at least \"min_playlist_tracks\"\n",
    "        # tracks, if \"min_playlist_tracks\" is at least 5, we are sure to find them.\n",
    "        selected_df = train[train['playlist_id'] == p].sample(5)\n",
    "\n",
    "        selected_tracks = selected_df['track_id'].values\n",
    "        target_tracks = np.union1d(target_tracks, selected_tracks)\n",
    "        indexes = np.union1d(indexes, selected_df.index.values)\n",
    "\n",
    "    test = train.loc[indexes].copy()\n",
    "    train = train.drop(indexes)\n",
    "\n",
    "    return train, test, pd.DataFrame(target_playlists, columns=['playlist_id']), pd.DataFrame(target_tracks, columns=['track_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import *\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def dot_with_top(m1, m2, def_rows_g, top=-1, row_group=1, similarity=\"dot\", shrinkage=0.000001, alpha=1):\n",
    "    \"\"\"\n",
    "        Produces the product between matrices m1 and m2.\n",
    "        Possible similarities: \"dot\", \"cosine\". By default it goes on \"dot\".\n",
    "        NB: Shrinkage is not implemented...\n",
    "        Code taken from\n",
    "            https://stackoverflow.com/questions/29647326/sparse-matrix-dot-product-keeping-only-n-max-values-per-result-row\n",
    "            and optimized for smart dot products.\n",
    "    \"\"\"\n",
    "    m2_transposed = m2.transpose()\n",
    "    \n",
    "    l2 = m2.sum(axis=0) #Â by cols\n",
    "    \n",
    "    if top > 0:\n",
    "        final_rows = []\n",
    "        row_id = 0\n",
    "        while row_id < m1.shape[0]:\n",
    "            last_row = row_id + row_group if row_id + row_group <= m1.shape[0] else m1.shape[0]\n",
    "            rows = m1[row_id:last_row]\n",
    "            if rows.count_nonzero() > 0:\n",
    "                if similarity == \"cosine-old\":\n",
    "                    res_rows = cosine_similarity(rows, m2_transposed, dense_output=False)\n",
    "                elif similarity == \"cosine\":\n",
    "                    res_rows = csr_matrix((np.dot(rows,m2) / (np.sqrt(rows.sum(axis=1)) * np.sqrt(l2) + shrinkage)))\n",
    "                elif similarity == \"cosine-asym\":\n",
    "                    res_rows = csr_matrix((np.dot(rows,m2) / (np.power(rows.sum(axis=1),alpha) * np.power(m2.sum(axis=0),(1-alpha)) + shrinkage)))\n",
    "                elif similarity == \"dot-old\":\n",
    "                    res_rows = rows.dot(m2)\n",
    "                else:\n",
    "                    res_rows = (np.dot(rows,m2) + shrinkage).toarray()\n",
    "                if res_rows.count_nonzero() > 0:\n",
    "                    for res_row in res_rows:\n",
    "                        if res_row.nnz > top:\n",
    "                            args_ids = np.argsort(res_row.data)[-top:]\n",
    "                            data = res_row.data[args_ids]\n",
    "                            cols = res_row.indices[args_ids]\n",
    "                            final_rows.append(csr_matrix((data, (np.zeros(top), cols)), shape=res_row.shape))\n",
    "                        else:\n",
    "                            args_ids = np.argsort(res_row.data)[-top:]\n",
    "                            data = res_row.data[args_ids]\n",
    "                            cols = res_row.indices[args_ids]\n",
    "                            final_rows.append(csr_matrix((data, (np.zeros(len(args_ids)), cols)), shape=res_row.shape))\n",
    "                            #print(\"Less than top: {0}\".format(len(args_ids)))\n",
    "                            #final_rows.append(def_rows_g[0])\n",
    "                else:\n",
    "                    print(\"Add empty 2\")\n",
    "                    for res_row in res_rows:\n",
    "                        final_rows.append(def_rows_g[0])\n",
    "            else:\n",
    "                print(\"Add empty 3\")\n",
    "                final_rows.append(def_rows_g)\n",
    "            row_id += row_group\n",
    "            if row_id % row_group == 0:\n",
    "                print(row_id)\n",
    "        return scipy.sparse.vstack(final_rows, 'csr')\n",
    "    return m1.dot(m2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_num_to_id(df, row_num, column = 'track_id'):\n",
    "    \"\"\" df must have a 'track_id' column \"\"\"\n",
    "    return df.iloc[row_num][column]\n",
    "\n",
    "def from_id_to_num(df, tr_id, column='track_id'):\n",
    "    \"\"\" df must have a 'track_id' column \"\"\"\n",
    "    return np.where(df[column].values == tr_id)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_final.csv', delimiter='\\t')\n",
    "playlists = pd.read_csv('data/playlists_final.csv', delimiter='\\t')\n",
    "target_playlists = pd.read_csv('data/target_playlists.csv', delimiter='\\t')\n",
    "target_tracks = pd.read_csv('data/target_tracks.csv', delimiter = '\\t')\n",
    "tracks = pd.read_csv('data/tracks_final.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We load them just to compare the ones for testing with the original ones.\n",
    "# NB: we shouldn't use them in training!\n",
    "train_original = pd.read_csv('data/train_final.csv', delimiter='\\t')\n",
    "target_playlists_original = pd.read_csv('data/target_playlists.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train), len(target_playlists), len(target_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, target_playlists, target_tracks = train_test_split(train, test_size=1, min_playlist_tracks=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train), len(test), len(target_playlists), len(target_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tracks[tracks[\"album\"] == \"[None]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tracks[tracks[\"album\"] == \"[]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tracks[tracks[\"album\"] == \"-1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Almost all of these were taken from one of your notebook, so you probably understand them\n",
    "tracks['track_id_tmp'] = tracks['track_id']\n",
    "\n",
    "tracks['track_id'] = tracks.index\n",
    "\n",
    "playlists['playlist_id_tmp'] = playlists['playlist_id']\n",
    "playlists['playlist_id'] = playlists.index\n",
    "\n",
    "train['playlist_id_tmp'] = train['playlist_id']\n",
    "train['track_id_tmp'] = train['track_id']\n",
    "\n",
    "track_to_num = pd.Series(tracks.index)\n",
    "track_to_num.index = tracks['track_id_tmp']\n",
    "\n",
    "playlist_to_num = pd.Series(playlists.index)\n",
    "playlist_to_num.index = playlists['playlist_id_tmp']\n",
    "\n",
    "num_to_tracks = pd.Series(tracks['track_id_tmp'])\n",
    "\n",
    "train['track_id'] = train['track_id'].apply(lambda x : track_to_num[x])\n",
    "train['playlist_id'] = train['playlist_id'].apply(lambda x : playlist_to_num[x])\n",
    "\n",
    "tracks.tags = tracks.tags.apply(lambda s: np.array(eval(s), dtype=int))\n",
    "\n",
    "playlists.title = playlists.title.apply(lambda s: np.array(eval(s), dtype=int))\n",
    "\n",
    "target_playlists['playlist_id_tmp'] = target_playlists['playlist_id']\n",
    "target_playlists['playlist_id'] = target_playlists['playlist_id'].apply(lambda x : playlist_to_num[x])\n",
    "\n",
    "target_tracks['track_id_tmp'] = target_tracks['track_id']\n",
    "target_tracks['track_id'] = target_tracks['track_id'].apply(lambda x : track_to_num[x])\n",
    "\n",
    "# Create a dataframe that maps a playlist to the set of its tracks\n",
    "playlist_tracks = pd.DataFrame(train['playlist_id'].drop_duplicates())\n",
    "playlist_tracks.index = train['playlist_id'].unique()\n",
    "playlist_tracks['track_ids'] = train.groupby('playlist_id').apply(lambda x : x['track_id'].values)\n",
    "playlist_tracks = playlist_tracks.sort_values('playlist_id')\n",
    "\n",
    "# Create a dataframe that maps a track to the set of the playlists it appears into\n",
    "track_playlists = pd.DataFrame(train['track_id'].drop_duplicates())\n",
    "track_playlists.index = train['track_id'].unique()\n",
    "track_playlists['playlist_ids'] = train.groupby('track_id').apply(lambda x : x['playlist_id'].values)\n",
    "track_playlists = track_playlists.sort_values('track_id')\n",
    "\n",
    "def transform_album_pr(alb):\n",
    "    global bad_albums\n",
    "    ar = eval(alb)\n",
    "    if len(ar) == 0:\n",
    "        return 2\n",
    "    elif ar[0] == None:\n",
    "        return 3\n",
    "    return 1\n",
    "\n",
    "tracks[\"album_presence\"] = tracks.album.apply(lambda alb: transform_album_pr(alb))\n",
    "\n",
    "# Substitute each bad album (i.e. an illformed album such as -1, None, etc) with the 0 album\n",
    "bad_albums = 0\n",
    "def transform_album_1(alb):\n",
    "    global bad_albums\n",
    "    ar = eval(alb)\n",
    "    if len(ar) == 0 or (len(ar) > 0 and (ar[0] == None or ar[0] == -1)):\n",
    "        ar = [0]\n",
    "        bad_albums += 1\n",
    "    return ar[0]\n",
    "\n",
    "tracks.album = tracks.album.apply(lambda alb: transform_album_1(alb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover albums\n",
    "Choose one of the following:<br>\n",
    "1 - fill with most similar albums according to the URM<br>\n",
    "2 - fill with brand new albums "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill with most similar albums according to the URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_UAM_album(tracks, playlist_tracks, target_playlists, norm=\"no\", OKAPI_K=1.7, OKAPI_B=0.75):\n",
    "    \"\"\"\n",
    "        Possible norms are \"no\", \"idf\", okapi\". Default to \"no\".\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_albums = tracks.album.unique()\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    UAM_album = lil_matrix((max(playlists.playlist_id)+1, max(unique_albums)+1))\n",
    "    UAM_album_no_norm = lil_matrix((max(playlists.playlist_id)+1, max(unique_albums)+1))\n",
    "    album_to_playlists = {}\n",
    "    \n",
    "    for row in playlist_tracks.itertuples():\n",
    "        pl_id = row.playlist_id\n",
    "        for tr_id in row.track_ids:\n",
    "            alb = tracks.loc[tr_id].album\n",
    "            UAM_album[pl_id,alb] += 1\n",
    "            UAM_album_no_norm[pl_id,alb] += 1\n",
    "            if alb not in album_to_playlists:\n",
    "                album_to_playlists[alb] = [pl_id]\n",
    "            else:\n",
    "                album_to_playlists[alb].append(pl_id)\n",
    "                \n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    album_to_val = {}\n",
    "    if norm == \"okapi\" or norm == \"idf\" or norm == \"tf\":\n",
    "        avg_document_length = functools.reduce(lambda acc,tr_ids: acc + len(tr_ids), playlist_tracks.track_ids, 0) / len(playlist_tracks)\n",
    "        N = len(playlist_tracks)\n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        for row in playlist_tracks.itertuples():\n",
    "            pl_id = row.playlist_id\n",
    "            albums = UAM_album.rows[pl_id]\n",
    "            data = UAM_album.data[pl_id]\n",
    "            for album in albums:\n",
    "                fq = UAM_album[pl_id,album]\n",
    "                nq = len(album_to_playlists[album])\n",
    "                idf = math.log(500/(nq + 0.5))\n",
    "                \n",
    "                if album not in album_to_val:\n",
    "                    album_to_val[album] = idf\n",
    "                    \n",
    "                if norm == \"idf\":\n",
    "                    UAM_album[pl_id,album] = idf\n",
    "                elif norm == \"okapi\":\n",
    "                    UAM_album[pl_id,album] = idf*(fq*(OKAPI_K+1))/(fq + OKAPI_K*(1 - OKAPI_B + OKAPI_B * sum(data) / avg_document_length))\n",
    "                elif norm == \"tf\":\n",
    "                    UAM_album[pl_id,album] = (fq*(OKAPI_K+1))/(fq + OKAPI_K*(1 - OKAPI_B + OKAPI_B * sum(data) / avg_document_length))\n",
    "            i += 1\n",
    "            if i % 1000 == 0:\n",
    "                print(i)\n",
    "    \n",
    "    return UAM_album, UAM_album_no_norm, album_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Substitute each album with the most similar album according to playlist frequencies\n",
    "UAM_album, UAM_album_no_norm, album_to_val = get_UAM_album(tracks, playlist_tracks, target_playlists, norm=\"idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracks.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracks[\"album_corrected\"] = tracks[\"album\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracks.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transform_album_sim(tr_id):\n",
    "    tot = np.zeros((1,max(tracks.album)+1))[0]\n",
    "    for pl_id in track_playlists.loc[tr_id].playlist_ids:\n",
    "        ar = UAM_album_no_norm[pl_id].toarray()[0]\n",
    "        tot += np.log(ar + 1)  \n",
    "        #tot += ar.clip(max=1)\n",
    "    if tot.max() != 0:\n",
    "        best_1 = tot.argmax()\n",
    "        best_2 = tot.argpartition(len(tot)-2)[-2]\n",
    "        if best_1 == 0:\n",
    "            return best_2\n",
    "    return 0\n",
    "\n",
    "corrected_albums = 0\n",
    "for row in tracks[tracks.track_id.isin(track_playlists.track_id)].itertuples():\n",
    "    if row.album_corrected == 0:\n",
    "        new_album = transform_album_sim(row.track_id)\n",
    "        if new_album != 0:\n",
    "            tracks.set_value(row.track_id, \"album_corrected\", new_album)\n",
    "            corrected_albums += 1\n",
    "            if corrected_albums % 100 == 0:\n",
    "                print(corrected_albums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracks.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill with brand new albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Substitute each 0 album with a brand new album\n",
    "def transform_album_2(alb):\n",
    "    global next_album_id\n",
    "    if alb == 0:\n",
    "        alb = next_album_id\n",
    "        next_album_id += 1\n",
    "    return alb\n",
    "last_album = tracks.album.max()\n",
    "next_album_id = last_album + 1\n",
    "tracks.album = tracks.album.apply(lambda alb: transform_album_2(alb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tracks[tracks.album == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover tags according to URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracks[\"tags_corrected\"] = tracks[\"tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count distinct tags\n",
    "tag_tracks = {}\n",
    "for row in tracks.itertuples():\n",
    "    for tag in row.tags:\n",
    "        if tag in tag_tracks:\n",
    "            tag_tracks[tag].append(row.track_id)\n",
    "        else:\n",
    "            tag_tracks[tag] = [row.track_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# User Tag Matrix UTM\n",
    "def get_UTM(tracks, playlist_tracks, tag_tracks, norm=\"no\", OKAPI_K=1.7, OKAPI_B=0.75, best_tag=False):\n",
    "    \"\"\"\n",
    "        Possible norm are \"no\", \"okapi\", \"idf\", \"tf\". Default to \"no\".\n",
    "    \"\"\"\n",
    "    \n",
    "    if best_tag:\n",
    "        unique_tags = list(best_tag_tracks.keys())\n",
    "    else:\n",
    "        unique_tags = list(tag_tracks.keys())\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    UTM = lil_matrix((max(playlists.playlist_id)+1, max(unique_tags)+1))\n",
    "    UTM_no_norm = lil_matrix((max(playlists.playlist_id)+1, max(unique_tags)+1))\n",
    "    \n",
    "    for row in playlist_tracks.itertuples():\n",
    "        pl_id = row.playlist_id\n",
    "        for tr_id in row.track_ids:\n",
    "            tr_row = tracks.loc[tr_id]\n",
    "            if best_tag:\n",
    "                UTM[pl_id,tr_row.best_tag] += 1\n",
    "                UTM_no_norm[pl_id,tr_row.best_tag] += 1\n",
    "            else:\n",
    "                for tag in tr_row.tags:\n",
    "                    UTM[pl_id,tag] += 1\n",
    "                    UTM_no_norm[pl_id,tag] += 1\n",
    "                \n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "    if norm == \"okapi\" or norm == \"idf\" or norm == \"tf\":\n",
    "        avg_document_length = sum(list(map(lambda l: sum(l), UTM.data)))/len(UTM.data)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for row in playlist_tracks.itertuples():\n",
    "            pl_id = row.playlist_id\n",
    "            tags = UTM.rows[pl_id]\n",
    "            data = UTM.data[pl_id]\n",
    "            for tag in tags:\n",
    "                fq = UTM[pl_id,tag]\n",
    "                if best_tag:\n",
    "                    nq = len(best_tag_tracks[tag])\n",
    "                else:\n",
    "                    nq = len(tag_tracks[tag])\n",
    "                idf = math.log(28000/(nq + 0.5))\n",
    "                \n",
    "                if norm == \"idf\":\n",
    "                    UTM[pl_id,tag] = idf\n",
    "                elif norm == \"okapi\":\n",
    "                    UTM[pl_id,tag] = idf*(fq*(OKAPI_K+1))/(fq + OKAPI_K*(1 - OKAPI_B + OKAPI_B * sum(data) / avg_document_length))\n",
    "                elif norm == \"tf\":\n",
    "                    UTM[pl_id,tag] = (fq*(OKAPI_K+1))/(fq + OKAPI_K*(1 - OKAPI_B + OKAPI_B * sum(data) / avg_document_length))\n",
    "                    \n",
    "            i += 1\n",
    "            if i % 1000 == 0:\n",
    "                print(i)\n",
    "    \n",
    "    return UTM, UTM_no_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "UTM, UTM_no_norm = get_UTM(tracks, playlist_tracks, tag_tracks, norm=\"okapi\", best_tag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tags_sim(tr_id):\n",
    "    tot = csr_matrix((1,max(tag_tracks)+1))\n",
    "    tr_row = track_playlists.loc[tr_id]\n",
    "    for pl_id in tr_row.playlist_ids:\n",
    "        tot += UTM[pl_id]\n",
    "    tot = tot.toarray()[0]\n",
    "    return tot.argsort()[::-1][0:5]\n",
    "    \n",
    "\n",
    "corrected_tags = 0\n",
    "for row in tracks[tracks.track_id.isin(track_playlists.track_id)].itertuples():\n",
    "    if len(row.tags) == 0:\n",
    "        new_tags = get_tags_sim(row.track_id)\n",
    "        tracks.set_value(row.track_id, \"tags_corrected\", new_tags)\n",
    "        \n",
    "        corrected_tags += 1\n",
    "        if corrected_tags % 100 == 0:\n",
    "            print(corrected_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracks.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-item similarity using only URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(gamma):\n",
    "    if gamma < 0:\n",
    "        return 1 - 1/(1 + math.exp(gamma))\n",
    "    else:\n",
    "        return 1/(1 + math.exp(-gamma))\n",
    "\n",
    "# User Rating Matrix URM\n",
    "def get_URM(tracks, playlists, playlist_tracks, track_playlists, norm=\"no\", pow_base=500, pow_exp=0.15):\n",
    "    \"\"\"\n",
    "        possible normalizations: \"no\", \"idf\", \"sqrt\", \"pow\", \"atan\".\n",
    "        Default \"no\".\n",
    "    \"\"\"\n",
    "    URM = lil_matrix((len(playlists), len(tracks)))\n",
    "    num_playlists = len(playlist_tracks)\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for row in track_playlists.itertuples():\n",
    "        track_id = row.track_id\n",
    "        nq = len(row.playlist_ids)\n",
    "        for pl_id in row.playlist_ids:\n",
    "            if norm == \"idf\":\n",
    "                URM[pl_id,track_id] = math.log((500)/nq)\n",
    "            elif norm == \"sqrt\":\n",
    "                URM[pl_id,track_id] = math.sqrt((500)/nq)\n",
    "            elif norm == \"pow\":\n",
    "                URM[pl_id,track_id] = math.pow((pow_base)/nq, pow_exp)\n",
    "            elif norm == \"atan\":\n",
    "                URM[pl_id,track_id] = 3 + 1*math.atan(-0.1*nq + 1)\n",
    "            else:\n",
    "                URM[pl_id,track_id] = 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "    \n",
    "    return URM\n",
    "\n",
    "#\n",
    "# URM:\n",
    "# \n",
    "#              tracks\n",
    "#            _________\n",
    "#           \\         \\\n",
    "# playlists \\         \\\n",
    "#           \\_________\\\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URM_normalize = get_URM(tracks, playlists, playlist_tracks, track_playlists, norm=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URM_pow = get_URM(tracks, playlists, playlist_tracks, track_playlists, norm=\"pow\", pow_base=500, pow_exp=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: produce item-item matrix with cosine similarity\n",
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, URM_normalize.shape[1]))#URM_pow.transpose()[0:row_group].dot(URM_pow) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "TTM_cosine = dot_with_top(URM_normalize.transpose(), URM_normalize, def_rows_i, top=50, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, URM_pow.shape[1]))#URM_pow.transpose()[0:row_group].dot(URM_pow) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "TTM_dot = dot_with_top(URM_pow.transpose(), URM_pow, def_rows_i, top=50, row_group=row_group, similarity=\"dot-old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-item similarity starting from a user-user similarity using only the URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, URM_normalize.transpose().shape[1]))#URM_pow.transpose()[0:row_group].dot(URM_pow) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "UUM_cosine = dot_with_top(URM_normalize, URM_normalize.transpose(), def_rows_i, top=500, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, UUM_cosine.transpose().shape[1]))#URM_pow.transpose()[0:row_group].dot(URM_pow) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "URM_UUM_cosine = dot_with_top(UUM_cosine, URM_normalize, def_rows_i, top=500, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, URM_UUM_cosine.shape[1]))#URM_pow.transpose()[0:row_group].dot(URM_pow) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "TTM_UUM_cosine = dot_with_top(URM_UUM_cosine.transpose(), URM_UUM_cosine, def_rows_i, top=50, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Calibration\n",
    "def calibrate_predictions(pred, theta=0.5):\n",
    "    max_r = np.amax(pred, axis=0)\n",
    "    mean_r = np.mean(pred, axis=0)\n",
    "\n",
    "    pred_coo = pred.tocoo()\n",
    "    pred_csr = pred.tocsr()\n",
    "    max_r_csr = max_r.tocsr()\n",
    "\n",
    "    counter = 0\n",
    "    for i,j,v in zip(pred_coo.row, pred_coo.col, pred_coo.data):\n",
    "        if v >= max_r_csr[0,j]:\n",
    "            pred_csr[i,j] = 1\n",
    "        elif v >= mean_r[0,j]:\n",
    "            pred_csr[i,j] = theta + (1 - theta)*((v - mean_r[0,j])/(max_r_csr[0,j] - mean_r[0,j]))\n",
    "        else:\n",
    "            pred_csr[i,j] = theta * v / mean_r[0,j]\n",
    "        counter += 1\n",
    "        if counter % 10000 == 0:\n",
    "            print(\"{0} out of {1}\".format(counter, len(pred.data)))\n",
    "    \n",
    "    return pred_csr\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Album"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using non-corrected album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_albums = tracks.album.unique()\n",
    "unique_albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "album_tracks = {}\n",
    "for row in tracks.itertuples():\n",
    "    if row.album in album_tracks:\n",
    "        album_tracks[row.album].append(row.track_id)\n",
    "    else:\n",
    "        album_tracks[row.album] = [row.track_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_IAM_album(tracks, target_tracks, norm=\"no\"):\n",
    "    \"\"\"\n",
    "        Possible norms are \"no\", \"idf\".\n",
    "        Default \"no\".\n",
    "    \"\"\"\n",
    "    unique_albums = tracks.album.unique()\n",
    "    IAM_album = lil_matrix((len(tracks), max(unique_albums)+1))\n",
    "    \n",
    "    num_tracks = len(tracks)\n",
    "    i = 0\n",
    "    \n",
    "    for row in tracks.itertuples():\n",
    "        if norm == \"idf\":\n",
    "            nq = len(album_tracks[row.album])\n",
    "            IAM_album[row.track_id,row.album] = math.log(500/(nq + 10))\n",
    "        else:\n",
    "            IAM_album[row.track_id,row.album] = 1\n",
    "        if i % 100 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "    \n",
    "    return IAM_album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IAM_album = get_IAM_album(tracks, target_tracks, norm=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SYM_ALBUM = IAM_album.dot(IAM_album.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artist\n",
    "Same steps as for Album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_artists = tracks.artist_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artist_tracks = {}\n",
    "for row in tracks.itertuples():\n",
    "    if row.artist_id in artist_tracks:\n",
    "        artist_tracks[row.artist_id].append(row.track_id)\n",
    "    else:\n",
    "        artist_tracks[row.artist_id] = [row.track_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Item Artist Matrix\n",
    "def get_IAM(tracks, target_tracks, norm=\"no\", n_best=5):\n",
    "    \"\"\"\n",
    "        Possible norms are \"no\", \"idf\". Default to \"no\".\n",
    "    \"\"\"\n",
    "    unique_artists = tracks.artist_id.unique()\n",
    "    IAM = lil_matrix((len(tracks), max(unique_artists)+1))\n",
    "    \n",
    "    num_tracks = len(tracks)\n",
    "    i = 0\n",
    "    \n",
    "    for row in tracks.itertuples():\n",
    "        if norm == \"idf\":\n",
    "            nq = len(artist_tracks[row.artist_id])\n",
    "            IAM[row.track_id,row.artist_id] = math.log(500/(nq + 0.5))\n",
    "        else:\n",
    "            IAM[row.track_id,row.artist_id] = 1\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "    \n",
    "    return IAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "IAM = get_IAM(tracks, target_tracks, norm=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3\n",
    "SYM_ARTIST = IAM.dot(IAM.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count distinct tags\n",
    "tag_tracks = {}\n",
    "for row in tracks.itertuples():\n",
    "    for tag in row.tags:\n",
    "        if tag in tag_tracks:\n",
    "            tag_tracks[tag].append(row.track_id)\n",
    "        else:\n",
    "            tag_tracks[tag] = [row.track_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Item Tag Matrix ITM\n",
    "def get_ITM(tracks, tag_tracks, norm=\"no\", best_tag=False):\n",
    "    \"\"\"\n",
    "        Possible norm are \"no\", \"idf\" and sqrt\". Default to \"no\".\n",
    "    \"\"\"\n",
    "    if best_tag:\n",
    "        unique_tags = list(best_tag_tracks.keys())\n",
    "    else:\n",
    "        unique_tags = list(tag_tracks.keys())\n",
    "    ITM = lil_matrix((len(tracks), max(unique_tags)+1))\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    tag_dict = tag_tracks\n",
    "        \n",
    "    for tag,track_ids in tag_dict.items():\n",
    "        nq = len(track_ids)\n",
    "        for track_id in track_ids:\n",
    "            if norm == \"idf\":\n",
    "                ITM[track_id,tag] = math.log(500/(nq + 1))\n",
    "            elif norm == \"sqrt\":\n",
    "                ITM[track_id,tag] = math.sqrt(500/(nq + 1))\n",
    "            else:\n",
    "                ITM[track_id,tag] = 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "    \n",
    "    return ITM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ITM = get_ITM(tracks, tag_tracks, norm=\"no\", best_tag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: produce item-item matrix with cosine similarity\n",
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, ITM.shape[0])) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "SYM_TAG = dot_with_top(ITM, ITM.transpose(), def_rows_i, top=50, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other similarities..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SYM_ALBUM_COMPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, SYM_ALBUM.shape[0])) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "TR_PL_ALBUM = dot_with_top(SYM_ALBUM, URM_normalize.transpose(), def_rows_i, top=200, row_group=row_group, similarity=\"cosine-old\")\n",
    "\n",
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, TR_PL_ALBUM.shape[0])) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "SYM_ALBUM_COMPLEX = dot_with_top(TR_PL_ALBUM, TR_PL_ALBUM.transpose(), def_rows_i, top=50, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SYM_ARTIST_COMPLEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, SYM_ARTIST.shape[0])) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "TR_PL_ARTIST = dot_with_top(SYM_ARTIST, URM_normalize.transpose(), def_rows_i, top=200, row_group=row_group, similarity=\"cosine-old\")\n",
    "\n",
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, TR_PL_ARTIST.shape[0])) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "SYM_ARTIST_COMPLEX = dot_with_top(TR_PL_ARTIST, TR_PL_ARTIST.transpose(), def_rows_i, top=50, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracks['owners'] = [np.array([]) for i in range(len(tracks))]\n",
    "\n",
    "for row in track_playlists.itertuples():\n",
    "    tr_id = row.track_id\n",
    "    owners = np.array([])\n",
    "    for pl_id in row.playlist_ids:\n",
    "        owners = np.concatenate((owners, [playlists.loc[pl_id].owner]))\n",
    "    tracks.set_value(tr_id, 'owners', owners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "owner_tracks = {}\n",
    "for row in tracks.itertuples():\n",
    "    for owner in row.owners:\n",
    "        if owner in owner_tracks:\n",
    "            owner_tracks[owner].append(row.track_id)\n",
    "        else:\n",
    "            owner_tracks[owner] = [row.track_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_owners = list(owner_tracks.keys())\n",
    "OTM = lil_matrix((len(tracks), max(unique_owners)+1))\n",
    "\n",
    "i = 0\n",
    "\n",
    "owner_dict = owner_tracks\n",
    "norm = \"no\"\n",
    "\n",
    "for owner,track_ids in owner_dict.items():\n",
    "    nq = len(track_ids)\n",
    "    for track_id in track_ids:\n",
    "        if norm == \"idf\":\n",
    "            OTM[track_id,owner] += math.log(500/(nq + 1))\n",
    "        elif norm == \"sqrt\":\n",
    "            OTM[track_id,owner] += math.sqrt(500/(nq + 1))\n",
    "        else:\n",
    "            OTM[track_id,owner] += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: produce item-item matrix with cosine similarity\n",
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, OTM.shape[0])) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "SYM_OWNERS = dot_with_top(OTM, OTM.transpose(), def_rows_i, top=50, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### playlist titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracks['title_tokens'] = [np.array([]) for i in range(len(tracks))]\n",
    "\n",
    "for row in track_playlists.itertuples():\n",
    "    tr_id = row.track_id\n",
    "    titles = np.array([])\n",
    "    for pl_id in row.playlist_ids:\n",
    "        titles = np.concatenate((titles, playlists.loc[pl_id].title))\n",
    "    tracks.set_value(tr_id, 'title_tokens', titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_tracks = {}\n",
    "for row in tracks.itertuples():\n",
    "    for title_token in row.title_tokens:\n",
    "        if title_token in title_tracks:\n",
    "            title_tracks[title_token].append(row.track_id)\n",
    "        else:\n",
    "            title_tracks[title_token] = [row.track_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_titles = list(title_tracks.keys())\n",
    "TTM_title = lil_matrix((len(tracks), max(unique_titles)+1))\n",
    "\n",
    "i = 0\n",
    "\n",
    "title_dict = title_tracks\n",
    "norm = \"no\"\n",
    "\n",
    "for title,track_ids in title_dict.items():\n",
    "    nq = len(track_ids)\n",
    "    for track_id in track_ids:\n",
    "        if norm == \"idf\":\n",
    "            TTM_title[track_id,title] += math.log(500/(nq + 1))\n",
    "        elif norm == \"sqrt\":\n",
    "            TTM_title[track_id,title] += math.sqrt(500/(nq + 1))\n",
    "        else:\n",
    "            TTM_title[track_id,title] += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: produce item-item matrix with cosine similarity\n",
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, TTM_title.shape[0])) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "SYM_TITLE = dot_with_top(TTM_title, TTM_title.transpose(), def_rows_i, top=50, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "U, S, V = svds(URM_normalize, k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S = np.diag(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M2 = np.dot(S, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_prediction_matrix_to_dataframe(pred_matrix, target_playlists, keep_best=5,\n",
    "                                       num_to_tracks={}, map_tracks=False):\n",
    "    pred_matrix_csr = pred_matrix.tocsr()\n",
    "\n",
    "    predictions = pd.DataFrame(target_playlists[:pred_matrix.shape[0]])\n",
    "    predictions.index = target_playlists['playlist_id'][:pred_matrix.shape[0]]\n",
    "    predictions['track_ids'] = [np.array([]) for i in range(len(predictions))]\n",
    "\n",
    "    for target_row,pl_id in enumerate(target_playlists.playlist_id[:pred_matrix.shape[0]]):\n",
    "        row_start = pred_matrix_csr.indptr[target_row]\n",
    "        row_end = pred_matrix_csr.indptr[target_row+1]\n",
    "        row_columns = pred_matrix_csr.indices[row_start:row_end]\n",
    "        row_data = pred_matrix_csr.data[row_start:row_end]\n",
    "\n",
    "        best_indexes = row_data.argsort()[::-1][:keep_best]\n",
    "        \n",
    "        pred = row_columns[best_indexes]\n",
    "        \n",
    "        if map_tracks:\n",
    "            pred = np.array([num_to_tracks[t] for t in pred])\n",
    "\n",
    "        predictions.loc[pl_id] = predictions.loc[pl_id].set_value('track_ids', pred)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SVDPredictor:\n",
    "    def __init__(self, name, m1, m2):\n",
    "        self.name = name\n",
    "        self.m1 = m1\n",
    "        self.m2 = m2\n",
    "        self.predictions = csr_matrix((0, self.m2.shape[1]))\n",
    "        self.maps = []\n",
    "        \n",
    "    def predict_group(self, row_start, row_end, target_playlists, target_tracks, keep_best=5,\n",
    "                      compute_MAP=False, test_good=None):\n",
    "        if not hasattr(self, 'ttracks'):\n",
    "            self.ttracks = list(set(target_tracks['track_id'].values))\n",
    "            \n",
    "        pl_group = target_playlists[row_start:row_end]\n",
    "\n",
    "        rows_URM = []\n",
    "        for pl_id in pl_group.playlist_id:\n",
    "            rows_URM += [csr_matrix(self.m1[pl_id,:])]\n",
    "        composed_URM = scipy.sparse.vstack(rows_URM, 'csr')\n",
    "\n",
    "        simil = np.array(composed_URM.dot(self.m2))\n",
    "        simil_to_save = simil.copy()\n",
    "\n",
    "        for i,pl_id in enumerate(pl_group.playlist_id):\n",
    "            row = simil[i]\n",
    "            pl_tracks = list(set(playlist_tracks.loc[pl_id]['track_ids']))\n",
    "            best_indexes = row.argsort()[::-1]\n",
    "            best_indexes = best_indexes[np.in1d(best_indexes, self.ttracks)] # keep only tracks that are in target_tracks\n",
    "            best_indexes = best_indexes[~np.in1d(best_indexes, pl_tracks)] # remove tracks that are already in the playlist\n",
    "            best_indexes = best_indexes[:keep_best] # keep only the best \n",
    "            new_row = np.zeros(len(row))\n",
    "            new_row[best_indexes] = row[best_indexes]\n",
    "            new_row_to_save = np.zeros(len(row))\n",
    "            new_row_to_save[best_indexes[:5]] = row[best_indexes[:5]]\n",
    "            simil[i] = new_row\n",
    "            simil_to_save[i] = new_row_to_save\n",
    "        \n",
    "        self.predictions = scipy.sparse.vstack([self.predictions, simil_to_save], 'csr')\n",
    "        \n",
    "        return csr_matrix(simil)\n",
    "    \n",
    "    def print_MAP(self, test_good, target_playlists, num_to_tracks):\n",
    "         predictions = from_prediction_matrix_to_dataframe(self.predictions, target_playlists, keep_best=5, num_to_tracks=num_to_tracks, map_tracks=True)\n",
    "         current_map = util.evaluate(test_good, predictions, should_transform_test=False)\n",
    "         print(\"{0}: {1}\".format(self.name, current_map))\n",
    "         self.maps.append(current_map)\n",
    "        \n",
    "    def get_predictors(self):\n",
    "        return [self]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimilarityPredictor:\n",
    "    def __init__(self, name, urm, similarity):\n",
    "        self.name = name\n",
    "        self.urm = urm\n",
    "        self.similarity = similarity\n",
    "        self.predictions = csr_matrix((0, self.urm.shape[1]))\n",
    "        self.maps = []\n",
    "        \n",
    "    def predict_group(self, row_start, row_end, target_playlists, target_tracks, keep_best=5,\n",
    "                      compute_MAP=False, test_good=None):\n",
    "        if not hasattr(self, 'ttracks'):\n",
    "            self.ttracks = list(set(target_tracks['track_id'].values))\n",
    "        \n",
    "        # \"pl_group\" is the set of the playlists that we want to make prediction for\n",
    "        pl_group = target_playlists[row_start:row_end]\n",
    "\n",
    "        rows_URM = []\n",
    "        for pl_id in pl_group.playlist_id:\n",
    "            rows_URM += [self.urm[pl_id,:]]\n",
    "        composed_URM = scipy.sparse.vstack(rows_URM, 'csr')\n",
    "\n",
    "        simil = np.array(np.divide(self.similarity.dot(composed_URM.transpose()).transpose().todense(), self.similarity.sum(axis=1).transpose() + 1))\n",
    "        simil_to_save = simil.copy()\n",
    "\n",
    "        for i,pl_id in enumerate(pl_group.playlist_id):\n",
    "            row = simil[i]\n",
    "            pl_tracks = list(set(playlist_tracks.loc[pl_id]['track_ids']))\n",
    "            best_indexes = row.argsort()[::-1]\n",
    "            best_indexes = best_indexes[np.in1d(best_indexes, self.ttracks)] # keep only tracks that are in target_tracks\n",
    "            best_indexes = best_indexes[~np.in1d(best_indexes, pl_tracks)] # remove tracks that are already in the playlist\n",
    "            best_indexes = best_indexes[:keep_best] # keep only the best \n",
    "            new_row = np.zeros(len(row))\n",
    "            new_row[best_indexes] = row[best_indexes]\n",
    "            new_row_to_save = np.zeros(len(row))\n",
    "            new_row_to_save[best_indexes[:5]] = row[best_indexes[:5]]\n",
    "            simil[i] = new_row\n",
    "            simil_to_save[i] = new_row_to_save\n",
    "        \n",
    "        self.predictions = scipy.sparse.vstack([self.predictions, simil_to_save], 'csr')\n",
    "        \n",
    "        return csr_matrix(simil)\n",
    "    \n",
    "    def print_MAP(self, test_good, target_playlists, num_to_tracks):\n",
    "         predictions = from_prediction_matrix_to_dataframe(self.predictions, target_playlists, keep_best=5, num_to_tracks=num_to_tracks, map_tracks=True)\n",
    "         current_map = util.evaluate(test_good, predictions, should_transform_test=False)\n",
    "         print(\"{0}: {1}\".format(self.name, current_map))\n",
    "         self.maps.append(current_map)\n",
    "        \n",
    "    def get_predictors(self):\n",
    "        return [self]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "class SumEnsemblePredictor:\n",
    "    def __init__(self, name, predictors, original_urm, weights=[]):\n",
    "        self.name = name\n",
    "        self.predictors = predictors\n",
    "        self.predictions = csr_matrix((0, original_urm.shape[1]))\n",
    "        self.original_urm = original_urm\n",
    "        if len(weights) == 0:\n",
    "            self.weights = [1 for p in predictors]\n",
    "        else:\n",
    "            self.weights = weights\n",
    "        self.maps = []\n",
    "        \n",
    "    def predict_group(self, row_start, row_end, target_playlists, target_tracks, keep_best=5, compute_MAP=False, test_good=None):\n",
    "        # \"pl_group\" is the set of the playlists that we want to make prediction for\n",
    "        pl_group = target_playlists[row_start:row_end]\n",
    "\n",
    "        # check fast mode: only if ensemble of SimilarityPredictor\n",
    "        ok_fast = False\n",
    "        for predictor in self.predictors:\n",
    "            if type(predictor) is not SimilarityPredictor or predictor.urm is not self.predictors[0].urm:\n",
    "                ok_fast = False\n",
    "                break\n",
    "                \n",
    "        if ok_fast:\n",
    "            if not hasattr(self, 'fast_predictor'):\n",
    "                print(\"Using SimilarityPredictor fast mode for {0}\".format(self.name))\n",
    "                symilarities = [p.similarity for p in self.predictors]\n",
    "                fast_sym = self.weights[0] * symilarities[0].tolil()\n",
    "                for i,s in enumerate(symilarities[1:]):\n",
    "                    fast_sym += self.weights[i+1] * s.tolil()\n",
    "                self.fast_predictor = SimilarityPredictor(\"fast_predictor\", predictors[0].urm, fast_sym)\n",
    "            \n",
    "            res_urm = self.fast_predictor.predict_group(row_start, row_end, target_playlists, target_tracks, keep_best=self.original_urm.shape[1],\n",
    "                                                        compute_MAP=False, test_good=None)\n",
    "        \n",
    "        else:\n",
    "            predictions = []\n",
    "            for predictor in self.predictors:\n",
    "                pred = predictor.predict_group(row_start, row_end, target_playlists, target_tracks, keep_best=self.original_urm.shape[1],\n",
    "                                                compute_MAP=compute_MAP, test_good=test_good)\n",
    "                if compute_MAP:\n",
    "                    predictor.print_MAP(test_good, target_playlists, num_to_tracks)\n",
    "                predictions.append(pred)\n",
    "            \n",
    "            res_urm = self.weights[0] * predictions[0].tolil()\n",
    "            for i,p in enumerate(predictions[1:]):\n",
    "                res_urm += self.weights[i+1] * p.tolil()\n",
    "            \n",
    "        res_urm_to_save = res_urm.copy()\n",
    "        \n",
    "        res_urm = res_urm.tolil()\n",
    "        res_urm_to_save = res_urm_to_save.tolil()\n",
    "            \n",
    "        for i,pl_id in enumerate(pl_group.playlist_id):\n",
    "            row = res_urm[i].toarray()[0]\n",
    "            best_indexes = row.argsort()[::-1]\n",
    "            best_indexes = best_indexes[:keep_best] # keep only the best\n",
    "            new_row = np.zeros(len(row))\n",
    "            new_row[best_indexes] = row[best_indexes]\n",
    "            new_row_to_save = np.zeros(len(row))\n",
    "            new_row_to_save[best_indexes[:5]] = row[best_indexes[:5]]\n",
    "            res_urm[i] = new_row\n",
    "            res_urm_to_save[i] = new_row_to_save\n",
    "        \n",
    "        self.predictions = scipy.sparse.vstack([self.predictions, res_urm_to_save], 'csr')\n",
    "        \n",
    "        return res_urm\n",
    "    \n",
    "    def print_MAP(self, test_good, target_playlists, num_to_tracks):\n",
    "         predictions = from_prediction_matrix_to_dataframe(self.predictions, target_playlists, keep_best=5, num_to_tracks=num_to_tracks, map_tracks=True)\n",
    "         current_map = util.evaluate(test_good, predictions, should_transform_test=False)\n",
    "         print(\"{0}: {1}\".format(self.name, current_map))\n",
    "         self.maps.append(current_map)\n",
    "    \n",
    "    def get_predictors(self):\n",
    "        return self.predictors + [self]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "\n",
    "def get_random_choice_with_probabilities(probabilities):\n",
    "    r = uniform(0,1)\n",
    "    acc = 0\n",
    "    for i,p in enumerate(probabilities):\n",
    "        acc += p\n",
    "        if r <= acc:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "class StochasticEnsemblePredictor:\n",
    "    def __init__(self, name, predictors, original_urm, probabilities=[]):\n",
    "        self.name = name\n",
    "        self.predictors = predictors\n",
    "        self.predictions = csr_matrix((0, original_urm.shape[1]))\n",
    "        self.original_urm = original_urm\n",
    "        self.probabilities = [p/sum(probabilities) for p in probabilities]\n",
    "        self.maps = []\n",
    "        \n",
    "    def predict_group(self, row_start, row_end, target_playlists, target_tracks, keep_best=5, compute_MAP=False, test_good=None):\n",
    "        # \"pl_group\" is the set of the playlists that we want to make prediction for\n",
    "        pl_group = target_playlists[row_start:row_end]\n",
    "\n",
    "        predictions = []\n",
    "        for predictor in self.predictors:\n",
    "            pred = predictor.predict_group(row_start, row_end, target_playlists, target_tracks, keep_best=original_urm.shape[1],\n",
    "                                            compute_MAP=compute_MAP, test_good=test_good)\n",
    "            if compute_MAP:\n",
    "                predictor.print_MAP(test_good, target_playlists, num_to_tracks)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        res_urm = lil_matrix(predictions[0].shape)\n",
    "        res_urm_to_save = res_urm.copy()\n",
    "\n",
    "        for i in range(predictions[0].shape[0]):\n",
    "            best_indexes = []\n",
    "            for p in predictions:\n",
    "                row = p[i].toarray()[0]\n",
    "                best_indexes.append(row.argsort()[::-1])\n",
    "            counters = [0 for j in best_indexes]\n",
    "\n",
    "            res_indexes = []\n",
    "            for j in range(0,keep_best):\n",
    "                c = get_random_choice_with_probabilities(self.probabilities)\n",
    "                new_index = best_indexes[c][counters[c]]\n",
    "                counters[c] += 1\n",
    "                while new_index in res_indexes:\n",
    "                    new_index = best_indexes[c][counters[c]]\n",
    "                    counters[c] += 1\n",
    "                res_indexes.append(new_index)\n",
    "\n",
    "\n",
    "            new_row = np.zeros(len(row))\n",
    "            new_row_to_save = np.zeros(len(row))\n",
    "            for j,idx in enumerate(res_indexes):\n",
    "                new_row[idx] = keep_best-j\n",
    "                if j < 5:\n",
    "                    new_row_to_save[idx] = keep_best-j\n",
    "\n",
    "            res_urm[i] = new_row\n",
    "            res_urm_to_save[i] = new_row_to_save\n",
    "        \n",
    "        self.predictions = scipy.sparse.vstack([self.predictions, res_urm_to_save], 'csr')\n",
    "        \n",
    "        return res_urm\n",
    "    \n",
    "    def print_MAP(self, test_good, target_playlists, num_to_tracks):\n",
    "         predictions = from_prediction_matrix_to_dataframe(self.predictions, target_playlists, keep_best=5, num_to_tracks=num_to_tracks, map_tracks=True)\n",
    "         current_map = util.evaluate(test_good, predictions, should_transform_test=False)\n",
    "         print(\"{0}: {1}\".format(self.name, current_map))\n",
    "         self.maps.append(current_map)\n",
    "    \n",
    "    def get_predictors(self):\n",
    "        return self.predictors + [self]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from recsys import utility as util\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def make_predictions(predictor, original_urm, target_playlists, target_tracks,\n",
    "                     row_group=1000,\n",
    "                     compute_MAP=False, test=None, num_to_tracks=\"nope\", graph_name=\"MAP\"):\n",
    "    if compute_MAP:\n",
    "        test_good = get_playlist_track_list2(test)\n",
    "        test_good.index = test_good.playlist_id.apply(lambda pl_id: playlist_to_num[pl_id])\n",
    "    else:\n",
    "        test_good = test\n",
    "    \n",
    "    res_urm = csr_matrix((0, original_urm.shape[1]))\n",
    "\n",
    "    row_start = 0\n",
    "    while row_start < len(target_playlists):\n",
    "        # We'll do dot products for all playlists in \"target_playlists\" from \"row_start\" to \"row_end\"\n",
    "        row_end = row_start + row_group if row_start + row_group <= len(target_playlists) else len(target_playlists)\n",
    "        \n",
    "        print(\"From {0} to {1}:\".format(row_start, row_end))\n",
    "\n",
    "        simil_urm = predictor.predict_group(row_start, row_end, target_playlists, target_tracks, keep_best=5,\n",
    "                                            compute_MAP=compute_MAP, test_good=test_good)\n",
    "        if compute_MAP:\n",
    "            predictor.print_MAP(test_good, target_playlists, num_to_tracks)\n",
    "            \n",
    "        print()\n",
    "\n",
    "        res_urm = scipy.sparse.vstack([res_urm, simil_urm], 'csr')\n",
    "        row_start = row_end\n",
    "        \n",
    "    if compute_MAP:\n",
    "        # show MAP graph\n",
    "        bins = int(len(target_playlists)/row_group)+1\n",
    "        x = [i/bins for i in range(0,bins)]\n",
    "        colors = [\"red\", \"green\", \"blue\", \"orange\", \"m\", \"gold\", \"c\", \"navy\", \"sienna\", \"grey\"]\n",
    "        predictors = predictor.get_predictors()\n",
    "        maps = [p.maps if len(p.maps)>0 else [0 for b in range(0,bins)] for p in predictors]\n",
    "        patches = []\n",
    "        for i,m in enumerate(maps):\n",
    "            plt.plot(x, m, colors[i])\n",
    "            patches.append(mpatches.Patch(color=colors[i], label=predictors[i].name))\n",
    "        plt.legend(handles=patches)\n",
    "\n",
    "        plt.xlabel('percentage of playlists considered')\n",
    "        plt.ylabel('MAP')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(graph_name + \".png\")\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    predictions = from_prediction_matrix_to_dataframe(res_urm, target_playlists, keep_best=5, num_to_tracks=num_to_tracks, map_tracks=True)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TTM_dot_copy = TTM_dot.copy()\n",
    "TTM_cosine_copy = TTM_cosine.copy()\n",
    "TTM_UUM_cosine_copy = TTM_UUM_cosine.copy()\n",
    "SYM_ALBUM_copy = SYM_ALBUM.copy()\n",
    "SYM_ARTIST_copy = SYM_ARTIST.copy()\n",
    "SYM_OWNERS_copy = SYM_OWNERS.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TTM_dot_copy = TTM_dot.copy()\n",
    "TTM_cosine_copy = TTM_cosine.copy()\n",
    "TTM_UUM_cosine_copy = TTM_UUM_cosine.copy()\n",
    "SYM_OWNERS_copy = SYM_OWNERS.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TTM_dot = TTM_dot_copy.copy()\n",
    "TTM_cosine = TTM_cosine_copy.copy()\n",
    "TTM_UUM_cosine = TTM_UUM_cosine_copy.copy()\n",
    "SYM_ALBUM = SYM_ALBUM_copy.copy()\n",
    "SYM_ARTIST = SYM_ARTIST_copy.copy()\n",
    "SYM_OWNERS = SYM_OWNERS_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "TTM_dot = normalize(TTM_dot, norm='l2', axis=0)\n",
    "TTM_cosine = normalize(TTM_cosine, norm='l2', axis=0)\n",
    "TTM_UUM_cosine = normalize(TTM_UUM_cosine, norm='l2', axis=0)\n",
    "SYM_ALBUM = normalize(SYM_ALBUM, norm='l1', axis=0)\n",
    "SYM_ARTIST = normalize(SYM_ARTIST, norm='l2', axis=0)\n",
    "SYM_OWNERS =  normalize(SYM_OWNERS, norm='l2', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ii_1 = SimilarityPredictor(\"ii_1\", URM_pow, TTM_dot)\n",
    "ii_2 = SimilarityPredictor(\"ii_2\", URM_pow, TTM_cosine)\n",
    "ii_3 = SimilarityPredictor(\"ii_3\", URM_pow, TTM_UUM_cosine)\n",
    "album_predictor = SimilarityPredictor(\"album\", URM_pow, SYM_ALBUM)\n",
    "artist_predictor = SimilarityPredictor(\"artist\", URM_pow, SYM_ARTIST)\n",
    "owner_predictor = SimilarityPredictor(\"owner\", URM_pow, SYM_OWNERS)\n",
    "#tag_predictor = SimilarityPredictor(\"tag\", URM_pow, SYM_TAG)\n",
    "#title_predictor = SimilarityPredictor(\"title\", URM_pow, SYM_TITLE)\n",
    "svd_predictor = SVDPredictor(\"svd\", U, M2)\n",
    "\n",
    "predictors = [ii_1, ii_3, album_predictor, artist_predictor, owner_predictor, svd_predictor]\n",
    "final_predictor = SumEnsemblePredictor(\"final_ens\", predictors, URM_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_predictions(final_predictor, URM_normalize, target_playlists, target_tracks,\n",
    "                 row_group=500,\n",
    "                 compute_MAP=True, test=test, num_to_tracks=num_to_tracks, graph_name=\"MAP_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_predictions(final_predictor, URM_normalize, target_playlists, target_tracks,\n",
    "                 row_group=500,\n",
    "                 compute_MAP=True, test=test, num_to_tracks=num_to_tracks, graph_name=\"MAP_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = make_predictions(final_predictor, URM_normalize, target_playlists, target_tracks,\n",
    "                 row_group=1000,\n",
    "                 compute_MAP=False, test=train, num_to_tracks=num_to_tracks, graph_name=\"MAP_mmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr_copy = predictions.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions['playlist_id'] = predictions['playlist_id_tmp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = predictions.drop(\"playlist_id_tmp\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the dataframe friendly for output -> convert np.array in string\n",
    "predictions['track_ids'] = predictions['track_ids'].apply(lambda x : ' '.join(map(str, x)))\n",
    "predictions.to_csv('results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
