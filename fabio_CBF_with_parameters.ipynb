{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import *\n",
    "from scipy.sparse.linalg import svds\n",
    "import math\n",
    "\n",
    "from recsys.preprocess import *\n",
    "\n",
    "import functools\n",
    "\n",
    "#from recsys.utility import *\n",
    "\n",
    "#RANDOM_STATE = 666\n",
    "\n",
    "#np.random.seed(RANDOM_STATE)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_test_split(train, test_size=0.3, min_playlist_tracks=7):\n",
    "    \"\"\"\n",
    "        Standard train_test_split, no modifications.\n",
    "    \"\"\"\n",
    "    playlists = train[train.playlist_id.isin(target_playlists_original.playlist_id)].groupby('playlist_id').count()\n",
    "\n",
    "    # Only playlists with at least \"min_playlist_tracks\" tracks are considered.\n",
    "    # If \"min_playlists_tracks\" = 7, then 28311 out of 45649 playlists in \"train\" are considered.\n",
    "    to_choose_playlists = playlists[playlists['track_id'] >= min_playlist_tracks].index.values\n",
    "\n",
    "\n",
    "    # Among these playlists, \"test_size * len(to_choose_playlists)\" distinct playlists are chosen for testing.\n",
    "    # If \"test_size\" = 0.3, then 8493 playlists are chosen for testing.\n",
    "    # It's a numpy array that contains playlis_ids.\n",
    "    target_playlists = np.random.choice(to_choose_playlists, replace=False, size=int(test_size * len(to_choose_playlists)))\n",
    "\n",
    "    target_tracks = np.array([])\n",
    "    indexes = np.array([])\n",
    "    for p in target_playlists:\n",
    "        # Choose 5 random tracks of such playlist: since we selected playlists with at least \"min_playlist_tracks\"\n",
    "        # tracks, if \"min_playlist_tracks\" is at least 5, we are sure to find them.\n",
    "        selected_df = train[train['playlist_id'] == p].sample(5)\n",
    "\n",
    "        selected_tracks = selected_df['track_id'].values\n",
    "        target_tracks = np.union1d(target_tracks, selected_tracks)\n",
    "        indexes = np.union1d(indexes, selected_df.index.values)\n",
    "\n",
    "    test = train.loc[indexes].copy()\n",
    "    train = train.drop(indexes)\n",
    "\n",
    "    return train, test, pd.DataFrame(target_playlists, columns=['playlist_id']), pd.DataFrame(target_tracks, columns=['track_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import *\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def dot_with_top(m1, m2, def_rows_g, top=-1, row_group=1, similarity=\"dot\", shrinkage=0.000001, alpha=1):\n",
    "    \"\"\"\n",
    "        Produces the product between matrices m1 and m2.\n",
    "        Possible similarities: \"dot\", \"cosine\". By default it goes on \"dot\".\n",
    "        NB: Shrinkage is not implemented...\n",
    "        Code taken from\n",
    "            https://stackoverflow.com/questions/29647326/sparse-matrix-dot-product-keeping-only-n-max-values-per-result-row\n",
    "            and optimized for smart dot products.\n",
    "    \"\"\"\n",
    "    m2_transposed = m2.transpose()\n",
    "    \n",
    "    l2 = m2.sum(axis=0) #Â by cols\n",
    "    \n",
    "    if top > 0:\n",
    "        final_rows = []\n",
    "        row_id = 0\n",
    "        while row_id < m1.shape[0]:\n",
    "            last_row = row_id + row_group if row_id + row_group <= m1.shape[0] else m1.shape[0]\n",
    "            rows = m1[row_id:last_row]\n",
    "            if rows.count_nonzero() > 0:\n",
    "                if similarity == \"cosine-old\":\n",
    "                    res_rows = cosine_similarity(rows, m2_transposed, dense_output=False)\n",
    "                elif similarity == \"cosine\":\n",
    "                    res_rows = csr_matrix((np.dot(rows,m2) / (np.sqrt(rows.sum(axis=1)) * np.sqrt(l2) + shrinkage)))\n",
    "                elif similarity == \"cosine-asym\":\n",
    "                    res_rows = csr_matrix((np.dot(rows,m2) / (np.power(rows.sum(axis=1),alpha) * np.power(m2.sum(axis=0),(1-alpha)) + shrinkage)))\n",
    "                elif similarity == \"dot-old\":\n",
    "                    res_rows = rows.dot(m2)\n",
    "                else:\n",
    "                    res_rows = (np.dot(rows,m2) + shrinkage).toarray()\n",
    "                if res_rows.count_nonzero() > 0:\n",
    "                    for res_row in res_rows:\n",
    "                        if res_row.nnz > top:\n",
    "                            args_ids = np.argsort(res_row.data)[-top:]\n",
    "                            data = res_row.data[args_ids]\n",
    "                            cols = res_row.indices[args_ids]\n",
    "                            final_rows.append(csr_matrix((data, (np.zeros(top), cols)), shape=res_row.shape))\n",
    "                        else:\n",
    "                            args_ids = np.argsort(res_row.data)[-top:]\n",
    "                            data = res_row.data[args_ids]\n",
    "                            cols = res_row.indices[args_ids]\n",
    "                            final_rows.append(csr_matrix((data, (np.zeros(len(args_ids)), cols)), shape=res_row.shape))\n",
    "                            #print(\"Less than top: {0}\".format(len(args_ids)))\n",
    "                            #final_rows.append(def_rows_g[0])\n",
    "                else:\n",
    "                    print(\"Add empty 2\")\n",
    "                    for res_row in res_rows:\n",
    "                        final_rows.append(def_rows_g[0])\n",
    "            else:\n",
    "                print(\"Add empty 3\")\n",
    "                final_rows.append(def_rows_g)\n",
    "            row_id += row_group\n",
    "            if row_id % row_group == 0:\n",
    "                print(row_id)\n",
    "        return scipy.sparse.vstack(final_rows, 'csr')\n",
    "    return m1.dot(m2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_predictions(test=None, target_playlists=None, urm=None,\n",
    "                     similarities=[], playlist_params=None,\n",
    "                     compute_MAP=False, row_group=100, verbose=False):\n",
    "    \"\"\"\n",
    "        Produces a prediction dataframe for \"test\", where each row corresponds to a playlist in \"target_playlists\".\n",
    "        If compute_MAP is true, then it print the MAP every \"row_group\" playlists.\n",
    "        It's optimized for doing dot products for different playlist at once.\n",
    "            \"row_group\" is the number of playlists in each of these optimized dot products.\n",
    "            The higher is row_group, the faster are the predictions but more memory is used.\n",
    "            \n",
    "        All predictions are done in the following way:\n",
    "            URM * (a * SYM_0 + b * SYM_1 + ...)\n",
    "            \n",
    "        Arguments:\n",
    "            - test: needed for computing MAP\n",
    "            - target_playlists: a dataframe containing the target playlists we want to predict for\n",
    "            - urm: the urm used for making predictions\n",
    "            - similarities: list with similarities used for making predictions\n",
    "            - playlist_params: dataframe containing parameters used for doing predictions. The name comvention\n",
    "                is the following: the parameter for SYM_0 is \"param_0\", etc...\n",
    "    \"\"\"\n",
    "    # Create predictions dataframe\n",
    "    predictions = pd.DataFrame(target_playlists)\n",
    "    predictions.index = target_playlists['playlist_id']\n",
    "    predictions['track_ids'] = [np.array([]) for i in range(len(predictions))]\n",
    "    predictions['track_ids_not_mapped'] = [np.array([]) for i in range(len(predictions))]\n",
    "    ttracks = set(target_tracks['track_id'].values)\n",
    "    if compute_MAP:\n",
    "        test_good = get_playlist_track_list2(test)\n",
    "        test_good.index = test_good.playlist_id.apply(lambda pl_id: playlist_to_num[pl_id])\n",
    "        print(len(test_good))\n",
    "    \n",
    "    # This is the sum of all the AP of the playlists.\n",
    "    # When we print the MAP, we divide \"sum_ap\" by the number of considered playlists.\n",
    "    sum_ap = 0\n",
    "    \n",
    "    # Let's start the predictions!\n",
    "    row_start = 0\n",
    "    while row_start < len(target_playlists):\n",
    "        # We'll do dot products for all playlists in \"target_playlists\" from \"row_start\" to \"row_end\"\n",
    "        row_end = row_start + row_group if row_start + row_group <= len(target_playlists) else len(target_playlists)\n",
    "        \n",
    "        # \"pl_group\" is the set of the playlists that we want to make prediction for\n",
    "        pl_group = target_playlists[row_start:row_end]\n",
    "        \n",
    "        # Now we need to build a matrix where, for each playlist in \"pl_group\", we take the correspondent URM row slice\n",
    "        rows_URM = []\n",
    "        for pl_id in pl_group.playlist_id:\n",
    "            rows_URM += [urm[pl_id,:]]\n",
    "        composed_URM = scipy.sparse.vstack(rows_URM, 'csr')\n",
    "        \n",
    "        # Compute predictions for current playlist group: here we do all the smart dot products...\n",
    "        simil_ar = []\n",
    "        for SYM in similarities:\n",
    "            simil_ar.append(np.array(np.divide(SYM.dot(composed_URM.transpose()).transpose().todense(), SYM.sum(axis=1).transpose() + 1)))\n",
    "            #simil_ar.append(np.array(SYM.dot(composed_URM.transpose()).transpose().todense()))\n",
    "            #simil_ar.append(np.array(cosine_similarity(SYM, composed_URM).transpose()))\n",
    "                            \n",
    "        # Now we should consider one playlist at a time, take its own personalized parameters and make the prediction\n",
    "        for i,pl_id in enumerate(pl_group.playlist_id):\n",
    "            # Tracks that we know are in the playlist (so we shouldn't recommend them)\n",
    "            pl_tracks = set(playlist_tracks.loc[pl_id]['track_ids'])\n",
    "            \n",
    "            # Retrieve parameters\n",
    "            params = []\n",
    "            for it,SYM in enumerate(similarities):\n",
    "                params.append(playlist_params.loc[pl_id][\"param_\" + str(it)])\n",
    "\n",
    "            simil = params[0] * simil_ar[0][i]\n",
    "            for p in range(1,len(simil_ar)):\n",
    "                simil += params[p] * simil_ar[p][i]\n",
    "            sorted_ind = simil.argsort()[::-1]\n",
    "\n",
    "            # Predict...\n",
    "            pred_not_mapped = []\n",
    "            pred = []\n",
    "            i = 0\n",
    "            while i < len(sorted_ind) and len(pred) < 5:\n",
    "                tr = sorted_ind[i]\n",
    "                if (tr in ttracks) and (tr not in pl_tracks) and (num_to_tracks[tr] not in pred):\n",
    "                    pred_not_mapped.append(tr)\n",
    "                    pred.append(num_to_tracks[tr])\n",
    "                i+=1\n",
    "            \n",
    "            predictions.loc[pl_id] = predictions.loc[pl_id].set_value('track_ids_not_mapped', np.array(pred_not_mapped))\n",
    "            predictions.loc[pl_id] = predictions.loc[pl_id].set_value('track_ids', np.array(pred))\n",
    "            \n",
    "            # Update MAP\n",
    "            if compute_MAP:\n",
    "                correct = 0\n",
    "                ap = 0\n",
    "                for it, t in enumerate(pred):\n",
    "                    tr_ids = test_good.loc[pl_id]['track_ids']\n",
    "                    if t in tr_ids:\n",
    "                        correct += 1\n",
    "                        ap += correct / (it+1)\n",
    "                ap /= len(pred)\n",
    "                sum_ap += ap\n",
    "        \n",
    "        # Update \"row_start\" to \"row_end\" and proceed to next pl_group\n",
    "        row_start = row_end\n",
    "        \n",
    "        print(row_start)\n",
    "        if compute_MAP:\n",
    "            print(sum_ap / row_start)\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_num_to_id(df, row_num, column = 'track_id'):\n",
    "    \"\"\" df must have a 'track_id' column \"\"\"\n",
    "    return df.iloc[row_num][column]\n",
    "\n",
    "def from_id_to_num(df, tr_id, column='track_id'):\n",
    "    \"\"\" df must have a 'track_id' column \"\"\"\n",
    "    return np.where(df[column].values == tr_id)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train_final.csv', delimiter='\\t')\n",
    "playlists = pd.read_csv('data/playlists_final.csv', delimiter='\\t')\n",
    "target_playlists = pd.read_csv('data/target_playlists.csv', delimiter='\\t')\n",
    "target_tracks = pd.read_csv('data/target_tracks.csv', delimiter = '\\t')\n",
    "tracks = pd.read_csv('data/tracks_final.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We load them just to compare the ones for testing with the original ones.\n",
    "# NB: we shouldn't use them in training!\n",
    "train_original = pd.read_csv('data/train_final.csv', delimiter='\\t')\n",
    "target_playlists_original = pd.read_csv('data/target_playlists.csv', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train), len(target_playlists), len(target_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test, target_playlists, target_tracks = train_test_split(train, test_size=1, min_playlist_tracks=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train), len(test), len(target_playlists), len(target_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Almost all of these were taken from one of your notebook, so you probably understand them\n",
    "tracks['track_id_tmp'] = tracks['track_id']\n",
    "\n",
    "tracks['track_id'] = tracks.index\n",
    "\n",
    "playlists['playlist_id_tmp'] = playlists['playlist_id']\n",
    "playlists['playlist_id'] = playlists.index\n",
    "\n",
    "train['playlist_id_tmp'] = train['playlist_id']\n",
    "train['track_id_tmp'] = train['track_id']\n",
    "\n",
    "track_to_num = pd.Series(tracks.index)\n",
    "track_to_num.index = tracks['track_id_tmp']\n",
    "\n",
    "playlist_to_num = pd.Series(playlists.index)\n",
    "playlist_to_num.index = playlists['playlist_id_tmp']\n",
    "\n",
    "num_to_tracks = pd.Series(tracks['track_id_tmp'])\n",
    "\n",
    "train['track_id'] = train['track_id'].apply(lambda x : track_to_num[x])\n",
    "train['playlist_id'] = train['playlist_id'].apply(lambda x : playlist_to_num[x])\n",
    "\n",
    "tracks.tags = tracks.tags.apply(lambda s: np.array(eval(s), dtype=int))\n",
    "\n",
    "playlists.title = playlists.title.apply(lambda s: np.array(eval(s), dtype=int))\n",
    "\n",
    "target_playlists['playlist_id_tmp'] = target_playlists['playlist_id']\n",
    "target_playlists['playlist_id'] = target_playlists['playlist_id'].apply(lambda x : playlist_to_num[x])\n",
    "\n",
    "target_tracks['track_id_tmp'] = target_tracks['track_id']\n",
    "target_tracks['track_id'] = target_tracks['track_id'].apply(lambda x : track_to_num[x])\n",
    "\n",
    "# Create a dataframe that maps a playlist to the set of its tracks\n",
    "playlist_tracks = pd.DataFrame(train['playlist_id'].drop_duplicates())\n",
    "playlist_tracks.index = train['playlist_id'].unique()\n",
    "playlist_tracks['track_ids'] = train.groupby('playlist_id').apply(lambda x : x['track_id'].values)\n",
    "playlist_tracks = playlist_tracks.sort_values('playlist_id')\n",
    "\n",
    "# Create a dataframe that maps a track to the set of the playlists it appears into\n",
    "track_playlists = pd.DataFrame(train['track_id'].drop_duplicates())\n",
    "track_playlists.index = train['track_id'].unique()\n",
    "track_playlists['playlist_ids'] = train.groupby('track_id').apply(lambda x : x['playlist_id'].values)\n",
    "track_playlists = track_playlists.sort_values('track_id')\n",
    "\n",
    "# Substitute each bad album (i.e. an illformed album such as -1, None, etc) with the 0 album\n",
    "bad_albums = 0\n",
    "def transform_album_1(alb):\n",
    "    global bad_albums\n",
    "    ar = eval(alb)\n",
    "    if len(ar) == 0 or (len(ar) > 0 and (ar[0] == None or ar[0] == -1)):\n",
    "        ar = [0]\n",
    "        bad_albums += 1\n",
    "    return ar[0]\n",
    "\n",
    "tracks.album = tracks.album.apply(lambda alb: transform_album_1(alb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover albums\n",
    "Choose one of the following:<br>\n",
    "1 - fill with most similar albums according to the URM<br>\n",
    "2 - fill with brand new albums "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill with most similar albums according to the URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_UAM_album(tracks, playlist_tracks, target_playlists, norm=\"no\", OKAPI_K=1.7, OKAPI_B=0.75):\n",
    "    \"\"\"\n",
    "        Possible norms are \"no\", \"idf\", okapi\". Default to \"no\".\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_albums = tracks.album.unique()\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    UAM_album = lil_matrix((max(playlists.playlist_id)+1, max(unique_albums)+1))\n",
    "    UAM_album_no_norm = lil_matrix((max(playlists.playlist_id)+1, max(unique_albums)+1))\n",
    "    album_to_playlists = {}\n",
    "    \n",
    "    for row in playlist_tracks.itertuples():\n",
    "        pl_id = row.playlist_id\n",
    "        for tr_id in row.track_ids:\n",
    "            alb = tracks.loc[tr_id].album\n",
    "            UAM_album[pl_id,alb] += 1\n",
    "            UAM_album_no_norm[pl_id,alb] += 1\n",
    "            if alb not in album_to_playlists:\n",
    "                album_to_playlists[alb] = [pl_id]\n",
    "            else:\n",
    "                album_to_playlists[alb].append(pl_id)\n",
    "                \n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    album_to_val = {}\n",
    "    if norm == \"okapi\" or norm == \"idf\" or norm == \"tf\":\n",
    "        avg_document_length = functools.reduce(lambda acc,tr_ids: acc + len(tr_ids), playlist_tracks.track_ids, 0) / len(playlist_tracks)\n",
    "        N = len(playlist_tracks)\n",
    "        \n",
    "        i = 0\n",
    "\n",
    "        for row in playlist_tracks.itertuples():\n",
    "            pl_id = row.playlist_id\n",
    "            albums = UAM_album.rows[pl_id]\n",
    "            data = UAM_album.data[pl_id]\n",
    "            for album in albums:\n",
    "                fq = UAM_album[pl_id,album]\n",
    "                nq = len(album_to_playlists[album])\n",
    "                idf = math.log(500/(nq + 0.5))\n",
    "                \n",
    "                if album not in album_to_val:\n",
    "                    album_to_val[album] = idf\n",
    "                    \n",
    "                if norm == \"idf\":\n",
    "                    UAM_album[pl_id,album] = idf\n",
    "                elif norm == \"okapi\":\n",
    "                    UAM_album[pl_id,album] = idf*(fq*(OKAPI_K+1))/(fq + OKAPI_K*(1 - OKAPI_B + OKAPI_B * sum(data) / avg_document_length))\n",
    "                elif norm == \"tf\":\n",
    "                    UAM_album[pl_id,album] = (fq*(OKAPI_K+1))/(fq + OKAPI_K*(1 - OKAPI_B + OKAPI_B * sum(data) / avg_document_length))\n",
    "            i += 1\n",
    "            if i % 1000 == 0:\n",
    "                print(i)\n",
    "    \n",
    "    return UAM_album, UAM_album_no_norm, album_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Substitute each album with the most similar album according to playlist frequencies\n",
    "UAM_album, UAM_album_no_norm, album_to_val = get_UAM_album(tracks, playlist_tracks, target_playlists, norm=\"idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracks.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracks[\"album_corrected\"] = tracks[\"album\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracks.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transform_album_sim(tr_id):\n",
    "    tot = np.zeros((1,max(tracks.album)+1))[0]\n",
    "    for pl_id in track_playlists.loc[tr_id].playlist_ids:\n",
    "        ar = UAM_album_no_norm[pl_id].toarray()[0]\n",
    "        tot += np.log(ar + 1)  \n",
    "        #tot += ar.clip(max=1)\n",
    "    if tot.max() != 0:\n",
    "        best_1 = tot.argmax()\n",
    "        best_2 = tot.argpartition(len(tot)-2)[-2]\n",
    "        if best_1 == 0:\n",
    "            return best_2\n",
    "    return 0\n",
    "\n",
    "corrected_albums = 0\n",
    "for row in tracks[tracks.track_id.isin(track_playlists.track_id)].itertuples():\n",
    "    if row.album_corrected == 0:\n",
    "        new_album = transform_album_sim(row.track_id)\n",
    "        if new_album != 0:\n",
    "            tracks.set_value(row.track_id, \"album_corrected\", new_album)\n",
    "            corrected_albums += 1\n",
    "            if corrected_albums % 100 == 0:\n",
    "                print(corrected_albums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_albums, corrected_albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracks.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tracks[tracks.album == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill with brand new albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Substitute each 0 album with a brand new album\n",
    "def transform_album_2(alb):\n",
    "    global next_album_id\n",
    "    if alb == 0:\n",
    "        alb = next_album_id\n",
    "        next_album_id += 1\n",
    "    return alb\n",
    "last_album = tracks.album.max()\n",
    "next_album_id = last_album + 1\n",
    "tracks.album = tracks.album.apply(lambda alb: transform_album_2(alb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tracks[tracks.album == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover tags according to URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracks[\"tags_corrected\"] = tracks[\"tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count distinct tags\n",
    "tag_tracks = {}\n",
    "for row in tracks.itertuples():\n",
    "    for tag in row.tags:\n",
    "        if tag in tag_tracks:\n",
    "            tag_tracks[tag].append(row.track_id)\n",
    "        else:\n",
    "            tag_tracks[tag] = [row.track_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# User Tag Matrix UTM\n",
    "def get_UTM(tracks, playlist_tracks, tag_tracks, norm=\"no\", OKAPI_K=1.7, OKAPI_B=0.75, best_tag=False):\n",
    "    \"\"\"\n",
    "        Possible norm are \"no\", \"okapi\", \"idf\", \"tf\". Default to \"no\".\n",
    "    \"\"\"\n",
    "    \n",
    "    if best_tag:\n",
    "        unique_tags = list(best_tag_tracks.keys())\n",
    "    else:\n",
    "        unique_tags = list(tag_tracks.keys())\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    UTM = lil_matrix((max(playlists.playlist_id)+1, max(unique_tags)+1))\n",
    "    UTM_no_norm = lil_matrix((max(playlists.playlist_id)+1, max(unique_tags)+1))\n",
    "    \n",
    "    for row in playlist_tracks.itertuples():\n",
    "        pl_id = row.playlist_id\n",
    "        for tr_id in row.track_ids:\n",
    "            tr_row = tracks.loc[tr_id]\n",
    "            if best_tag:\n",
    "                UTM[pl_id,tr_row.best_tag] += 1\n",
    "                UTM_no_norm[pl_id,tr_row.best_tag] += 1\n",
    "            else:\n",
    "                for tag in tr_row.tags:\n",
    "                    UTM[pl_id,tag] += 1\n",
    "                    UTM_no_norm[pl_id,tag] += 1\n",
    "                \n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "    if norm == \"okapi\" or norm == \"idf\" or norm == \"tf\":\n",
    "        avg_document_length = sum(list(map(lambda l: sum(l), UTM.data)))/len(UTM.data)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for row in playlist_tracks.itertuples():\n",
    "            pl_id = row.playlist_id\n",
    "            tags = UTM.rows[pl_id]\n",
    "            data = UTM.data[pl_id]\n",
    "            for tag in tags:\n",
    "                fq = UTM[pl_id,tag]\n",
    "                if best_tag:\n",
    "                    nq = len(best_tag_tracks[tag])\n",
    "                else:\n",
    "                    nq = len(tag_tracks[tag])\n",
    "                idf = math.log(28000/(nq + 0.5))\n",
    "                \n",
    "                if norm == \"idf\":\n",
    "                    UTM[pl_id,tag] = idf\n",
    "                elif norm == \"okapi\":\n",
    "                    UTM[pl_id,tag] = idf*(fq*(OKAPI_K+1))/(fq + OKAPI_K*(1 - OKAPI_B + OKAPI_B * sum(data) / avg_document_length))\n",
    "                elif norm == \"tf\":\n",
    "                    UTM[pl_id,tag] = (fq*(OKAPI_K+1))/(fq + OKAPI_K*(1 - OKAPI_B + OKAPI_B * sum(data) / avg_document_length))\n",
    "                    \n",
    "            i += 1\n",
    "            if i % 1000 == 0:\n",
    "                print(i)\n",
    "    \n",
    "    return UTM, UTM_no_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "UTM, UTM_no_norm = get_UTM(tracks, playlist_tracks, tag_tracks, norm=\"okapi\", best_tag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tags_sim(tr_id):\n",
    "    tot = csr_matrix((1,max(tag_tracks)+1))\n",
    "    tr_row = track_playlists.loc[tr_id]\n",
    "    for pl_id in tr_row.playlist_ids:\n",
    "        tot += UTM[pl_id]\n",
    "    tot = tot.toarray()[0]\n",
    "    return tot.argsort()[::-1][0:5]\n",
    "    \n",
    "\n",
    "corrected_tags = 0\n",
    "for row in tracks[tracks.track_id.isin(track_playlists.track_id)].itertuples():\n",
    "    if len(row.tags) == 0:\n",
    "        new_tags = get_tags_sim(row.track_id)\n",
    "        tracks.set_value(row.track_id, \"tags\", new_tags)\n",
    "        \n",
    "        corrected_tags += 1\n",
    "        if corrected_tags % 100 == 0:\n",
    "            print(corrected_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracks.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Training\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-item similarity using only URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(gamma):\n",
    "    if gamma < 0:\n",
    "        return 1 - 1/(1 + math.exp(gamma))\n",
    "    else:\n",
    "        return 1/(1 + math.exp(-gamma))\n",
    "\n",
    "# User Rating Matrix URM\n",
    "def get_URM(tracks, playlists, playlist_tracks, track_playlists, norm=\"no\", pow_base=500, pow_exp=0.15):\n",
    "    \"\"\"\n",
    "        possible normalizations: \"no\", \"idf\", \"sqrt\", \"pow\", \"atan\".\n",
    "        Default \"no\".\n",
    "    \"\"\"\n",
    "    URM = lil_matrix((len(playlists), len(tracks)))\n",
    "    num_playlists = len(playlist_tracks)\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    for row in track_playlists.itertuples():\n",
    "        track_id = row.track_id\n",
    "        nq = len(row.playlist_ids)\n",
    "        for pl_id in row.playlist_ids:\n",
    "            if norm == \"idf\":\n",
    "                URM[pl_id,track_id] = math.log((500)/nq)\n",
    "            elif norm == \"sqrt\":\n",
    "                URM[pl_id,track_id] = math.sqrt((500)/nq)\n",
    "            elif norm == \"pow\":\n",
    "                URM[pl_id,track_id] = math.pow((pow_base)/nq, pow_exp)\n",
    "            elif norm == \"atan\":\n",
    "                URM[pl_id,track_id] = 3 + 1*math.atan(-0.1*nq + 1)\n",
    "            else:\n",
    "                URM[pl_id,track_id] = 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "    \n",
    "    return URM\n",
    "\n",
    "#\n",
    "# URM:\n",
    "# \n",
    "#              tracks\n",
    "#            _________\n",
    "#           \\         \\\n",
    "# playlists \\         \\\n",
    "#           \\_________\\\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URM_normalize = get_URM(tracks, playlists, playlist_tracks, track_playlists, norm=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URM_pow = get_URM(tracks, playlists, playlist_tracks, track_playlists, norm=\"pow\", pow_base=500, pow_exp=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: produce item-item matrix with cosine similarity\n",
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, URM_normalize.shape[1]))#URM_pow.transpose()[0:row_group].dot(URM_pow) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "TTM_cosine = dot_with_top(URM_normalize.transpose(), URM_normalize, def_rows_i, top=20, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, URM_pow.shape[1]))#URM_pow.transpose()[0:row_group].dot(URM_pow) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "TTM_dot = dot_with_top(URM_pow.transpose(), URM_pow, def_rows_i, top=20, row_group=row_group, similarity=\"dot-old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-item similarity starting from a user-user similarity using only the URM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, URM_normalize.transpose().shape[1]))#URM_pow.transpose()[0:row_group].dot(URM_pow) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "UUM_cosine = dot_with_top(URM_normalize, URM_normalize.transpose(), def_rows_i, top=200, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, UUM_cosine.transpose().shape[1]))#URM_pow.transpose()[0:row_group].dot(URM_pow) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "URM_UUM_cosine = dot_with_top(UUM_cosine, URM_normalize, def_rows_i, top=500, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, URM_UUM_cosine.shape[1]))#URM_pow.transpose()[0:row_group].dot(URM_pow) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "TTM_UUM_cosine = dot_with_top(URM_UUM_cosine.transpose(), URM_UUM_cosine, def_rows_i, top=20, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Calibration\n",
    "def calibrate_predictions(pred, theta=0.5):\n",
    "    max_r = np.amax(pred, axis=0)\n",
    "    mean_r = np.mean(pred, axis=0)\n",
    "\n",
    "    pred_coo = pred.tocoo()\n",
    "    pred_csr = pred.tocsr()\n",
    "    max_r_csr = max_r.tocsr()\n",
    "\n",
    "    counter = 0\n",
    "    for i,j,v in zip(pred_coo.row, pred_coo.col, pred_coo.data):\n",
    "        if v >= max_r_csr[0,j]:\n",
    "            pred_csr[i,j] = 1\n",
    "        elif v >= mean_r[0,j]:\n",
    "            pred_csr[i,j] = theta + (1 - theta)*((v - mean_r[0,j])/(max_r_csr[0,j] - mean_r[0,j]))\n",
    "        else:\n",
    "            pred_csr[i,j] = theta * v / mean_r[0,j]\n",
    "        counter += 1\n",
    "        if counter % 10000 == 0:\n",
    "            print(\"{0} out of {1}\".format(counter, len(pred.data)))\n",
    "    \n",
    "    return pred_csr\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Album\n",
    "\n",
    "<div style=\"white-space: pre-wrap;\">\n",
    "Steps:\n",
    "1 - Compute the playlists_x_albums (i.e. the UAM_album matrix, where U stands for User) sparse matrix. I do this before computing the tracks_x_albums (i.e. the IAM_album matrix, where I stands for Item) sparse matrix because here I compute also the \"album_to_val\" dictionary, which contains the IDF value of each album obtained considering the playlists as document (and not the tracks). However at the moment I don't use this because I compute the IAM_album matrix without any normalization, so you may skip it...\n",
    "2 - Compute the tracks_x_albums IAM_album sparse matrix.\n",
    "3 - Compute the SYM_ALBUM tracks_x_tracks matrix by doing IAM_album.dot(IAM_album.transpose()). It's not big, so I don't need to keep the K best values...\n",
    "4 - Compute the album_parameter, which means \"how much each playlist is affine to album similarity\". I do this by computing the entropy of the numpy array containing the occurrences of the albums in the playlist, and then doing 1/(entropy_of_array + 0.05).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_albums = tracks.album.unique()\n",
    "unique_albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "album_tracks = {}\n",
    "for row in tracks.itertuples():\n",
    "    if row.album in album_tracks:\n",
    "        album_tracks[row.album].append(row.track_id)\n",
    "    else:\n",
    "        album_tracks[row.album] = [row.track_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_IAM_album(tracks, target_tracks, norm=\"no\", most_similar=5):\n",
    "    \"\"\"\n",
    "        Possible norms are \"no\", \"idf\", \"most-similar\".\n",
    "        Default \"no\".\n",
    "    \"\"\"\n",
    "    unique_albums = tracks.album.unique()\n",
    "    IAM_album = lil_matrix((len(tracks), max(unique_albums)+1))\n",
    "    \n",
    "    num_tracks = len(tracks)\n",
    "    i = 0\n",
    "    \n",
    "    if norm == \"most-similar\":\n",
    "        def get_album_sim(alb, n_best=5):\n",
    "            bests = []\n",
    "            a = ALB_ALB_SYM[alb].toarray()[0]\n",
    "            for i in range(n_best):\n",
    "                bests.append(a.argpartition(len(a)-1-i)[-1-i])\n",
    "            return bests\n",
    "\n",
    "        for row in tracks[tracks.track_id.isin(track_playlists.track_id)].itertuples():\n",
    "            bests = get_album_sim(row.album, n_best=5)\n",
    "            for it,alb in enumerate(bests):\n",
    "                IAM_album[row.track_id, alb] = 1 - it*0.1\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            i += 1\n",
    "            \n",
    "    else:\n",
    "        for row in tracks.itertuples():\n",
    "            nq = 1\n",
    "            if norm == \"idf\":\n",
    "                nq = len(album_tracks[row.album])\n",
    "                if row.album in album_to_val:\n",
    "                    IAM_album[row.track_id,row.album] = math.log(500/(nq + 0.5))\n",
    "                else:\n",
    "                    IAM_album[row.track_id,row.album] = 0 # Give zero if the album is not in any playlist!\n",
    "            else:\n",
    "                IAM_album[row.track_id,row.album] = 1\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            i += 1\n",
    "    \n",
    "    return IAM_album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "IAM_album = get_IAM_album(tracks, target_tracks, norm=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SYM_ALBUM = IAM_album.dot(IAM_album.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artist\n",
    "Same steps as for Album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_artists = tracks.artist_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Item Artist Matrix\n",
    "def get_IAM(tracks, target_tracks, norm=\"no\", n_best=5):\n",
    "    \"\"\"\n",
    "        Possible norms are \"no\", \"idf\", \"most-similar\". Default to \"no\".\n",
    "    \"\"\"\n",
    "    unique_artists = tracks.artist_id.unique()\n",
    "    IAM = lil_matrix((len(tracks), max(unique_artists)+1))\n",
    "    \n",
    "    num_tracks = len(tracks)\n",
    "    i = 0\n",
    "    \n",
    "    if norm == \"most-similar\":\n",
    "        def get_artist_sim(art, n_best=5):\n",
    "            bests = []\n",
    "            a = ART_ART_SYM[art].toarray()[0]\n",
    "            for i in range(n_best):\n",
    "                bests.append(a.argpartition(len(a)-1-i)[-1-i])\n",
    "            return bests\n",
    "\n",
    "        for row in tracks[tracks.track_id.isin(track_playlists.track_id)].itertuples():\n",
    "            bests = get_artist_sim(row.artist_id, n_best=5)\n",
    "            for it,art in enumerate(bests):\n",
    "                IAM[row.track_id, art] = 1 - it*0.1\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            i += 1\n",
    "    else:\n",
    "        for row in tracks.itertuples():\n",
    "            if norm == \"idf\":\n",
    "                if row.artist_id in artist_to_val:\n",
    "                    IAM[row.track_id,row.artist_id] = artist_to_val[row.artist_id]\n",
    "                else:\n",
    "                    IAM[row.track_id,row.artist_id] = 0 # Give zero if the album is not in any playlist!\n",
    "            else:\n",
    "                IAM[row.track_id,row.artist_id] = 1\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(i)\n",
    "            i += 1\n",
    "    \n",
    "    return IAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "IAM = get_IAM(tracks, target_tracks, norm=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3\n",
    "SYM_ARTIST = IAM.dot(IAM.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Item Tag Matrix ITM\n",
    "def get_ITM(tracks, tag_tracks, norm=\"no\", best_tag=False):\n",
    "    \"\"\"\n",
    "        Possible norm are \"no\", \"sqrt\", okapi\". Default to \"no\".\n",
    "    \"\"\"\n",
    "    if best_tag:\n",
    "        unique_tags = list(best_tag_tracks.keys())\n",
    "    else:\n",
    "        unique_tags = list(tag_tracks.keys())\n",
    "    ITM = lil_matrix((len(tracks), max(unique_tags)+1))\n",
    "    \n",
    "    num_tracks = len(tracks)\n",
    "    i = 0\n",
    "    \n",
    "    if best_tag:\n",
    "        tag_dict = best_tag_tracks\n",
    "    else:\n",
    "        tag_dict = tag_tracks\n",
    "        \n",
    "    for tag,track_ids in tag_dict.items():\n",
    "        nq = len(track_ids)\n",
    "        for track_id in track_ids:\n",
    "            if norm == \"okapi\":\n",
    "                ITM[track_id,tag] = math.log((num_tracks - nq + 0.5)/(nq + 0.5))\n",
    "            elif norm == \"sqrt\":\n",
    "                ITM[track_id,tag] = math.sqrt((num_tracks - nq + 0.5)/(nq + 0.5))\n",
    "            else:\n",
    "                ITM[track_id,tag] = 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "    \n",
    "    return ITM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ITM = get_ITM(tracks, tag_tracks, norm=\"no\", best_tag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: produce item-item matrix with cosine similarity\n",
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, ITM.shape[0])) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "SYM_TAG = dot_with_top(ITM, ITM.transpose(), def_rows_i, top=25, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other similarities..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "UAM_album, UAM_album_no_norm, album_to_val = get_UAM_album(tracks, playlist_tracks, target_playlists, norm=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: produce item-item matrix with cosine similarity\n",
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, UAM_album.shape[0]))#IAM_album[0:row_group].dot(UAM_album.transpose()) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "TR_PL_ALBUM = dot_with_top(IAM_album, UAM_album.transpose(), def_rows_i, top=10, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: produce item-item matrix with cosine similarity\n",
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, TR_PL_ALBUM.shape[0]))#TR_PL_ALBUM[0:row_group].dot(TR_PL_ALBUM.transpose()) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "SYM_ALBUM_COMPLEX = dot_with_top(TR_PL_ALBUM, TR_PL_ALBUM.transpose(), def_rows_i, top=10, row_group=row_group, similarity=\"cosine-old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# User Artist Matrix UAM\n",
    "def get_UAM(tracks, playlist_tracks, target_playlists, norm=\"no\", OKAPI_K=1.7, OKAPI_B=0.75):\n",
    "    \"\"\"\n",
    "        Possible norms are \"no\", \"idf\", okapi\". Default to \"no\".\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_artists = tracks.artist_id.unique()\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    UAM = lil_matrix((max(playlists.playlist_id)+1, max(unique_artists)+1))\n",
    "    UAM_no_norm = lil_matrix((max(playlists.playlist_id)+1, max(unique_artists)+1))\n",
    "    artist_to_playlists = {}\n",
    "    \n",
    "    for row in playlist_tracks.itertuples():\n",
    "        pl_id = row.playlist_id\n",
    "        for tr_id in row.track_ids:\n",
    "            art = tracks.loc[tr_id].artist_id\n",
    "            UAM[pl_id,art] += 1\n",
    "            UAM_no_norm[pl_id,art] += 1\n",
    "            if art not in artist_to_playlists:\n",
    "                artist_to_playlists[art] = [pl_id]\n",
    "            else:\n",
    "                artist_to_playlists[art].append(pl_id)\n",
    "                \n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "    \n",
    "    artist_to_val = {}\n",
    "    if norm == \"okapi\" or norm == \"idf\":\n",
    "        avg_document_length = functools.reduce(lambda acc,tr_ids: acc + len(tr_ids), playlist_tracks.track_ids, 0) / len(playlist_tracks)\n",
    "        N = len(playlist_tracks)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        for row in playlist_tracks.itertuples():\n",
    "            pl_id = row.playlist_id\n",
    "            artists = UAM.rows[pl_id]\n",
    "            data = UAM.data[pl_id]\n",
    "            for artist in artists:\n",
    "                fq = UAM[pl_id,artist]\n",
    "                nq = len(artist_to_playlists[artist])\n",
    "                idf = math.log((N - nq + 0.5)/(nq + 0.5))\n",
    "                \n",
    "                if artist not in artist_to_val:\n",
    "                    artist_to_val[artist] = idf\n",
    "                \n",
    "                if norm == \"idf\":\n",
    "                    UAM[pl_id,artist] = idf\n",
    "                else:\n",
    "                    UAM[pl_id,artist] = idf*(fq*(OKAPI_K+1))/(fq + OKAPI_K*(1 - OKAPI_B + OKAPI_B * sum(data) / avg_document_length))\n",
    "            i += 1\n",
    "            if i % 1000 == 0:\n",
    "                print(i)\n",
    "    \n",
    "    return UAM, UAM_no_norm, artist_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "UAM, UAM_no_norm, artist_to_val = get_UAM(tracks, playlist_tracks, target_playlists, norm=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: produce item-item matrix with cosine similarity\n",
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, UAM.shape[0]))#IAM[0:row_group].dot(UAM.transpose()) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "TR_PL_ARTIST = dot_with_top(IAM, UAM.transpose(), def_rows_i, top=10, row_group=row_group, similarity=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: produce item-item matrix with cosine similarity\n",
    "row_group = 1000\n",
    "def_rows_i = csr_matrix((row_group, TR_PL_ARTIST.shape[0]))#TR_PL_ARTIST[0:row_group].dot(TR_PL_ARTIST.transpose()) # this is needed to fill some rows that would be all zeros otherwise...\n",
    "SYM_ARTIST_COMPLEX = dot_with_top(TR_PL_ARTIST, TR_PL_ARTIST.transpose(), def_rows_i, top=10, row_group=row_group, similarity=\"cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare matrices for schwiftyness!\n",
    "Yoooo we're gonna get schwiftyyyyy: print all the info necessaries for get_schwifty.cpp to work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_similarity(matrix_to_print, directory, filename):\n",
    "    matrix_to_print = matrix_to_print.tocoo()\n",
    "\n",
    "    file = open(directory + \"/\" + filename + \".txt\",\"w\") \n",
    "\n",
    "    rows = matrix_to_print.row;\n",
    "    cols = matrix_to_print.col;\n",
    "    data = matrix_to_print.data;\n",
    "\n",
    "    file.write(\"{0} {1}\\n\".format(matrix_to_print.shape[0], matrix_to_print.shape[1]))\n",
    "\n",
    "    for i in range(0,len(rows)):\n",
    "        file.write(\"{0} \".format(rows[i]))\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "    for i in range(0,len(cols)):\n",
    "        file.write(\"{0} \".format(cols[i]))\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "    for i in range(0,len(data)):\n",
    "        file.write(\"{0} \".format(data[i]))\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_URM(urm, directory, filename):\n",
    "    file = open(directory + \"/\" + filename + \".txt\",\"w\")\n",
    "    \n",
    "    urm_coo = urm.tocoo()\n",
    "\n",
    "    rows = urm_coo.row;\n",
    "    cols = urm_coo.col;\n",
    "    data = urm_coo.data;\n",
    "\n",
    "    file.write(\"{0} {1}\\n\".format(urm_coo.shape[0], urm_coo.shape[1]))\n",
    "\n",
    "    for i in range(0,len(rows)):\n",
    "        file.write(\"{0} \".format(rows[i]))\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "    for i in range(0,len(rows)):\n",
    "        file.write(\"{0} \".format(cols[i]))\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "    for i in range(0,len(rows)):\n",
    "        file.write(\"{0} \".format(data[i]))\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "    for pl_id in target_playlists.playlist_id:\n",
    "        file.write(\"{0} \".format(pl_id))\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_test(urm, directory, filename):\n",
    "    file = open(directory + \"/\" + filename + \".txt\",\"w\")\n",
    "    \n",
    "    urm_coo = urm.tocoo()\n",
    "\n",
    "    rows = urm_coo.row;\n",
    "    cols = urm_coo.col;\n",
    "\n",
    "    file.write(\"{0} {1}\\n\".format(urm_coo.shape[0], urm_coo.shape[1]))\n",
    "\n",
    "    for i in range(0,len(rows)):\n",
    "        file.write(\"{0} \".format(rows[i]))\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "    for i in range(0,len(rows)):\n",
    "        file.write(\"{0} \".format(cols[i]))\n",
    "    file.write(\"\\n\")\n",
    "\n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_target_playlists(target_playlists, directory, filename):\n",
    "    file = open(directory + \"/\" + filename + \".txt\",\"w\")\n",
    "    \n",
    "    file.write(\"{0}\\n\".format(len(target_playlists)))\n",
    "    for pl_id in target_playlists.playlist_id:\n",
    "        file.write(\"{0} \".format(pl_id))\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_target_tracks(target_tracks, directory, filename):\n",
    "    file = open(directory + \"/\" + filename + \".txt\",\"w\")\n",
    "    \n",
    "    file.write(\"{0}\\n\".format(len(target_tracks)))\n",
    "    for tr_id in target_tracks.track_id:\n",
    "        file.write(\"{0} \".format(tr_id))\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "    file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schwifty_directory = \"test1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_URM(URM_pow, schwifty_directory, \"tracks_in_playlist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"test['playlist_id_tmp'] = test['playlist_id']\n",
    "test['track_id_tmp'] = test['track_id']\n",
    "test['track_id'] = test['track_id'].apply(lambda x : track_to_num[x])\n",
    "test['playlist_id'] = test['playlist_id'].apply(lambda x : playlist_to_num[x])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Create a dataframe that maps a playlist to the set of its tracks\n",
    "playlist_tracks_test = pd.DataFrame(test['playlist_id'].drop_duplicates())\n",
    "playlist_tracks_test.index = test['playlist_id'].unique()\n",
    "playlist_tracks_test['track_ids'] = test.groupby('playlist_id').apply(lambda x : x['track_id'].values)\n",
    "playlist_tracks_test = playlist_tracks_test.sort_values('playlist_id')\n",
    "\n",
    "# Create a dataframe that maps a track to the set of the playlists it appears into\n",
    "track_playlists_test = pd.DataFrame(test['track_id'].drop_duplicates())\n",
    "track_playlists_test.index = test['track_id'].unique()\n",
    "track_playlists_test['playlist_ids'] = test.groupby('track_id').apply(lambda x : x['playlist_id'].values)\n",
    "track_playlists_test = track_playlists_test.sort_values('track_id')\n",
    "\n",
    "URM_test = get_URM(tracks, playlists, playlist_tracks_test, track_playlists_test, norm=\"no\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print_test(URM_test, schwifty_directory, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print_target_tracks(target_tracks, schwifty_directory, \"target_tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print_target_playlists(target_playlists, schwifty_directory, \"target_playlists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TTM_cosine.data = np.power(TTM_cosine.data, 0.75)\n",
    "print_similarity(TTM_cosine, schwifty_directory, \"similarity_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TTM_dot.data = (np.power(TTM_dot.data, 0.18) - 1)\n",
    "print_similarity(TTM_dot, schwifty_directory, \"similarity_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_similarity(TTM_UUM_cosine, schwifty_directory, \"similarity_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_similarity(SYM_ALBUM, schwifty_directory, \"similarity_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_similarity(SYM_ARTIST, schwifty_directory, \"similarity_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "\n",
    "def load_playlist_params(location, params_bitmask):\n",
    "    content = None\n",
    "    with open(os.path.join(location, 'playlist_params.txt'), 'r') as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "    playlist_params = pd.DataFrame(playlists.playlist_id)\n",
    "    p = 0\n",
    "    for it,ch in enumerate(params_bitmask):\n",
    "        param_name = \"param_\" + str(it)\n",
    "        if ch == \"1\":\n",
    "            p_list = list(map(float, content[p].strip().split(' ')))\n",
    "            playlist_params[param_name] = Series(data=p_list, index=playlist_params.index)\n",
    "            p += 1\n",
    "        else:\n",
    "            playlist_params[param_name] = 0\n",
    "\n",
    "    return playlist_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "call([\"./get_schwifty\", \"test1\", \"11111\", \"adadelta\", \"500\", \"0.9\", \"0.25\", \"100\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "playlist_params = load_playlist_params(\"test1\", \"11111\")\n",
    "playlist_params[playlist_params.playlist_id.isin(target_playlists.playlist_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similarities = [TTM_cosine, TTM_dot, TTM_UUM_cosine, SYM_ALBUM, SYM_ARTIST]\n",
    "\n",
    "predictions = make_predictions(test=train, target_playlists=target_playlists, urm=URM_pow,\n",
    "                 similarities=similarities, playlist_params=playlist_params,\n",
    "                 compute_MAP=False, row_group=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr_copy = predictions.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions['playlist_id'] = predictions['playlist_id_tmp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = predictions.drop(\"playlist_id_tmp\", axis=1)\n",
    "predictions = predictions.drop(\"track_ids_not_mapped\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the dataframe friendly for output -> convert np.array in string\n",
    "predictions['track_ids'] = predictions['track_ids'].apply(lambda x : ' '.join(map(str, x)))\n",
    "predictions.to_csv('results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
